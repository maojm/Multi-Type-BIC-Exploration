\pagebreak
\section{Introduction}

Recommendation systems are ubiquitous in modern online economy. Well-known examples include recommendations for movies (\eg  Netflix), products (\eg  Amazon), and restaurants (\eg  Yelp).
%, and vacations (\eg Tripadvisor).
High-quality recommendations are a crucial part of the value proposition of the respective business.
%
A typical recommendation system encourages its users to submit evaluations and/or reviews of their experiences, and aggregates this feedback in order to provide better recommendations to future users. Thus, each user plays a dual rule: she consumes information from the previous users (indirectly, via recommendations),
% and other information revealed by the system),
and produces new information (\eg  a review) that benefits future users. This dual role creates a tension between exploration, exploitation, and users' incentives.

%On a very abstract level, users of a recommendation system make decisions, \eg which movie to watch or which restaurant to go to, facing uncertainty about the outcomes of these decisions.
A social planner -- a hypothetical entity that controls users for the sake of common good -- would balance ``exploration" vs.  ``exploitation", \ie  trying out insufficiently known alternatives for the sake of acquiring new information vs. making myopic decisions based on this information. Designing algorithms to trade off these two objectives is a well-researched subject in machine learning and operations research.
%A common term for this subject is \emph{multi-armed bandits}.
%
However, user incentives with respect to exploration are misaligned with those of society. To the extent that a given user decides to ``explore", she experiences all the downside of this decision (a potentially suboptimal experience), whereas the upside (improved recommendations) is spread over many users in the future. Absent an adequate mechanism to compensate for the risks of exploration, self-interested users have incentives exploit. This may result in outcomes that are very suboptimal. First, it may take much longer to arrive at good recommendations. Second, the recommendations may consistently suffer from selection bias in the data: \eg  ratings of a particular movie may mostly come from people who like this type or genre. Third, in some natural but idealized models (\eg  \cite{Kremer-JPE14,ICexploration-ec15}), there are simple  examples when optimal recommendations are never found because the corresponding actions are never taken.

Thus, we have a problem of \emph{incentivizing exploration}.
%: creating incentives for self-interested users to explore so as to benefit others.
Relying on monetary incentives or the natural human propensity for exploration can be financially and technologically infeasible, and/or introduce selection bias.
%
%While monetary incentives such as discounts are sometimes used for this purpose, they are often prohibitively expensive and/or may result in additional selection bias, as the feedback would come from users that are more sensitive to the said discounts. Likewise, just relying on a natural human propensity for exploration creates a different selection bias, as the data is skewed in favor of users who are more adventurous.
%
A recent line of work, started by \cite{Kremer-JPE14}, instead strives to incentivize exploration by taking advantage of the inherent \emph{information asymmetry} -- the fact that the recommendation system has more information than a typical user -- by restricting the information revealed to the agents. These papers posit a simple model, termed \emph{Bayesian Exploration} in \cite{ICexplorationGames-ec16}. The recommendation system is a ``principal" that interacts with a stream of self-interested ``agents" arriving one by one. Each agent needs to make a decision: take an action from a given set of alternatives. The principal issues a recommendation, %perhaps along with some other information,
and observes the outcome, but cannot direct the agent to take a particular action. The problem is to design a ``recommendation policy" for the principal that learns over time to make good recommendations \emph{and} ensures that the agents are incentivized to follow this recommendation.
%Essentially, this is a multi-armed bandit problem with substantial constraints due to the user incentives.
A single round of this model is a version of a well-known ``Bayesian Persuasion game" \cite{Kamenica-aer11}.

\xhdr{Our scope.}
We depart from prior work on Bayesian Exploration in that we allow the agents to have different preferences from one another. The preferences of an agent are encapsulated in her {\em type}, \eg vegan vs meat-lover.
%The details of the model are as follows.
When an agent takes a particular action, the outcome depends the action itself (\eg the selection of restaurant), the ``state" of the world (\eg the qualities of the restaurants), and the type of the agent. The state is persistent (does not change over time), but initially not known; a Bayesian prior on the state is common knowledge. In each round, the agent type is drawn independently from a fixed and known distribution. The principal strives to learn the best possible recommendation for each agent type.

We consider three models, depending on whether and when the agent type is revealed to the principal: the type is revealed immediately after the agent arrives (\emph{public types}), the type is revealed only after the principal issues a recommendation (\emph{reported types}),\footnote{Reported types may arise if
%agents are myopic and
the principal asks agents to report the type after the recommendation is issued, \eg in a survey. While the agents are allowed to misreport their respective types, they have no incentives to do that.}
and the type is never revealed (\emph{private types}).
%
We design a near-optimal recommendation policy for each modeling choice. The ``benchmark" that our policies compete with is the ``best-in-hindsight"
policy: a recommendation policy whose Bayesian-expected reward is optimal for a particular problem instance.

\OMIT{In fact, we consider a stronger benchmark: optimal Bayesian-expected reward achieved by any recommendation policy in any one round.}

\xhdr{Explorability and public types.}
A distinctive feature of Bayesian Exploration is that it may be impossible to incentivize some of the agent types to take some of the actions, no matter what the principal does or how much time she has. This feature has dramatic implications on the design of recommendation policies, as well as on the benchmarks that these policies are compared to.

For a more succinct terminology, a given type-action pair is called \emph{explorable} if it is explored (\ie an agent of this type takes this action) under some recommendation policy in some round with positive probability. Then we also say that this action is explorable for this type. Thus: some actions might not be explorable for a given type. Moreover, the set of all explorable type-action pairs may depend on the state of the world, so  exploring one type-action pair may inform the principal as to whether some other type-action pair can be explored.

In terms of benchmarks, the best one could hope for is the ``best explorable action" for a particular agent type: an explorable action with a largest reward in the realized state of the world. This is in stark contrast to the situation without agents' incentives, where one can realistically compete with the best type-dependent action among all actions.

Our recommendation policy for public types competes with this ``best explorable action" benchmark, matching its Bayesian-expected reward in a long run. While it is not difficult to prove that there exists a recommendation policy that eventually matches this benchmark, producing such a policy as a computational procedure, let alone a computationally efficient one, is non-trivial.

Our policy focuses on exploring all explorable type-action pairs, allowing us to compute the best explorable action for each type. Exploration completes in a ``constant" number of rounds, in the sense that it depends on the problem instance but not the time horizon. We do not attempt to further optimize second-order measures such as regret or convergence rate.

Exploration needs to be done gradually, whereby exploring one action may enable the policy to explore another. In fact, exploring some action for one type may enable the policy to explore some other action for another type. Our policy proceeds in phases: for each type, we explore all actions that can possibly be explored with information available in the beginning of the phase. Phases are synchronized across types: essentially, each type waits for all other types to complete.

An important building block is the analysis of the single-round game. We use information theory to characterize how much information the principal has collected. In particular, we prove a version of \emph{information monotonicity}: informally, the set of explorable type-action pairs set is larger if the principal has more information about the state.


\xhdr{Beyond public types.}
For private types or reported types, recommending one particular action to the current agent is not very meaningful because the agents' type is not known at the time the recommendation is issued. Instead, one can recommend a \emph{menu}: a mapping from agent types to actions. Without loss of generality, we restrict to  Bayesian-incentive compatible (BIC) policies: essentially, policies that output menus such that the agents are incentivized to follow them.

The explorability issue resurfaces, albeit in a different formal setting. A menu is called \emph{explorable} if some BIC policy recommends this menu in some round with a positive probability. As some menus might not be explorable, we are interested in the ``best explorable menu": an explorable menu with a largest expected reward for the realized state of the world.

Our recommendation policy for private types competes with the ``best explorable menu", eventually matching its Bayesian-expected reward. As for public types, it is not difficult to prove that some recommendation policy can compete with this benchmark, the challenge is to produce such policy as a computational procedure. Our policy focuses on exploring all explorable menus, and proceeds gradually, whereby exploring one menu may enable the policy to explore another.

One additional difficulty is that exploring a given menu does not immediately reveal the reward of a given type-action pair (because multiple types could map to the same action). Even keeping track of what the policy knows is now non-trivial. Consequently, the analysis of a single-round game becomes more involved, as one needs to argue about ``approximate information-monotonicity".

Our policy for reported types does much better: we show that in the long run it matches our benchmark for public types. This may seem counterintuitive because ``reported types" are completely useless to the principal in the single-round game (whereas public types are very useful). Essentially, we reduce the problem to the public types case, at the cost of a much longer exploration.

\xhdr{Comparative statics for explorability.}
We focus on the \emph{explorable set} -- the set of all explorable type-action pairs -- and study how  it is affected by the model choice and the diversity of types.

First, let us fix an arbitrary instance of Bayesian Exploration, and vary the model choice of public vs. reported vs. private types. We find that the explorable set stays the same if we transition from public types to reported types, and either stays the same or decreases (becomes a strict subset) if we transition from reported types to private types. We provide a concrete example when the latter transition makes a huge difference.

Second, let us take the same problem instance and vary the distribution $\DT$ of agent types. For public types (and therefore also for reported types), we find that the explorable set is determined by the support set of $\DT$. Further, if we increase the support set, \ie transition to another distribution whose support set is a strict superset of that of $\DT$, then the explorable set stays the same or increases. In other words, \emph{diversity of agent types helps exploration}. We provide a concrete example when the explorable set increases very substantially even if the support set increases by a single type. However, for private types the picture is quite different: we provide an example when \emph{diversity hurts}, in the same sense as above. 