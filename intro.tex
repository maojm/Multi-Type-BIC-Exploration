\section{Introduction}

Recommendation systems are ubiquitous in modern online economy. Well-known examples include recommendations for movies (\eg  Netflix), products (\eg  Amazon), restaurants (\eg  Yelp), and vacations (\eg Tripadvisor).
and so on. High-quality recommendations are often a crucial part of the value proposition of the respective business. 

A typical recommendation system encourages its users to submit evaluations and/or reviews of their experiences, and aggregates this feedback in order to provide better recommendations to future users. Thus, each user plays a dual rule: she consumes information from the previous users (indirectly, via recommendations and other information revealed by the system), and may produce new information (\eg  a review) that may benefit future users. This dual role creates a tension between exploration, exploitation, and users' incentives.

On a very abstract level, users of a recommendation system make decisions, \eg which movie to watch or which restaurant to go to, facing uncertainty about the outcomes of these decisions. A social planner -- a hypothetical entity that controls users for the sake of common good -- would balance ``exploration" vs.  ``exploitation", \ie  trying out insufficiently known alternatives for the sake of acquiring new information vs. making myopic decisions based on this information. Designing algorithms to trade off these two objectives is a well-researched subject in machine learning and operations research. A common term for this subject is \emph{multi-armed bandits}.

However, user incentives with respect to exploration are misaligned with those of the society. To the extent that a given user decides to ``explore", she experiences all the downside of this decision (a potentially suboptimal experience), whereas the upside (improved recommendations) is spread over many users in the future. Absent an adequate mechanism to compensate for the risks of exploration, self-interested users have incentives to lean in favor of exploitation. This may result in outcomes that are very suboptimal. First, it may take much longer to arrive at good recommendations. Second, the recommendations may consistently suffer from selection bias in the data: \eg  ratings of a particular movie may mostly come from people who like this type or genre. Third, in some natural but idealized models (\eg  \cite{Kremer-JPE14,ICexploration-ec15}), there are simple  examples when optimal recommendations are never found because the corresponding actions are never taken.

Thus, we have a problem of \emph{incentivizing exploration}: creating incentives for self-interested users to explore so as to benefit others. While monetary incentives such as discounts are sometimes used for this purpose, they are often prohibitively expensive and/or may result in additional selection bias, as the feedback would come from users that are more sensitive to the said discounts. Likewise, just relying on a natural human propensity for exploration creates a different selection bias, as the data is skewed in favor of users who are more adventurous.

A recent line of work, started by \cite{Kremer-JPE14}, strives to incentivize exploration without incurring these biases. The idea is to restrict the information revealed to the agents, and take advantage of \emph{information asymmetry}: the fact that the recommendation system has more information than a typical user. These papers posit a simple model, termed \emph{Bayesian Exploration} in \cite{ICexplorationGames-ec16}. The recommendation system is a ``principal" that interacts with a stream of self-interested ``agents" arriving one by one. Each agent needs to make a decision: take an action from a given set of alternatives. The principal issues a recommendation, perhaps along with some other information, and observes the outcome, but cannot direct the agent to take a particular action. The problem is to design a ``recommendation policy" for the principal that learns over time to make good recommendations \emph{and} ensures that the agents are incentivized to follow. Essentially, this is a multi-armed bandit problem with substantial constraints due to the user incentives. A single round of this model is a well-known ``Bayesian Persuasion game" \cite{Kamenica-aer11}.

\xhdr{Our scope and results.}
We depart from prior work on Bayesian Exploration in that we allow the agents to have different preferences from one time step to another. When an agent takes a particular action, the outcome depends on three things: the action itself, the ``state" of the world, and the ``type" of an agent. The state is persistent (does not change over time), but initially not known; Bayesian prior on the state is common knowledge. The principal strives to learn the best possible recommendation for each agent type.

We consider three modeling choices, depending on whether and when the agent type is revealed to the principal: the type is revealed immediately (\emph{public types}), the type is revealed only after the principal issues a recommendation (\emph{reported types}), and the type is never revealed (\emph{private types}). We design a near-optimal recommendation policy for each modeling choice. 

The ``benchmark" that our policies compete with is the ``best-in-hindsight"
policy: a recommendation policy whose Bayesian-expected reward is optimal for a particular problem instance.  

\OMIT{In fact, we consider a stronger benchmark: optimal Bayesian-expected reward achieved by any recommendation policy in any one round.}

\xhdr{Explorability and public types.}
A distinctive feature of Bayesian Exploration is that it may be impossible to incentivize some of the agent types to take some of the actions, no matter what the principal does or how much time she has. This feature has dramatic implications on the design of recommendation policies, as well as on the benchmarks that these policies are compared to.

For a more succinct terminology, a given type-action pair is \emph{explored} when an agent of this type takes this action. The pair is \emph{explorable} if it is explored under some recommendation policy in some round with positive probability. An action is \emph{explorable} for a given agent type if the corresponding type-action pair is explorable. Thus, some actions might not be explorable for a given type.  

In terms of benchmarks, the best one could hope for is the ``best explorable action" for a particular agent type: an explorable action with a largest reward in the realized state of the world. This is in stark contrast to the situation without agents' incentives, where one can realistically compete with the best type-dependent action among all actions.

Our recommendation policy for public types competes with this ``best explorable action" benchmark, matching its Bayesian-expected reward in a long run. While it is not difficult to prove that there exists a recommendation policy that eventually matches this benchmark, producing such policy as a computational procedure, let alone a computationally efficient one, is very non-trivial.

Our recommendation policy focuses on exploring all explorable type-action pairs (which allows to compute the best explorable action for a given type). Exploration needs to be done in stages, whereby exploring one type-action pair may enable the principal to explore another. Moreover, the set of all explorable actions may depend on the state of the world, and so  exploring one type-action pair may inform the principal whether another type-action pair is explorable. 

We accomplish exploration in a ``constant" number of rounds, in the sense that it that depends on the problem instance but not the time horizon. Beyond that, we do not attempt to optimize the second-order measures such as regret or convergence rate.

\xhdr{Beyond public types.}
For private types or reported types, recommending one particular action to the current agent is not very meaningful because the agents' type is not known at the time the recommendation is issued. Instead, one can recommend a \emph{menu}: a mapping from agent types to actions. Without loss of generality, we restrict to  Bayesian-incentive compatible (BIC) policies: essentially, policies that output menus such that the agents are incentivized to follow them.

The explorability issue resurfaces, albeit in a different formal setting. A menu is called \emph{explorable} if some BIC policy recommends this menu in some round with a positive probability. As some menus might not be explorable, we are interested in the ``best explorable menu": an explorable menu with a largest expected reward for the realized state of the world.

Our recommendation policy for private types competes with the ``best explorable menu", eventually matching its Bayesian-expected reward. As for public types, it is not difficult to prove that some recommendation policy can compete with this benchmark, the challenge is to produce such policy as a computational procedure.
 
 
\ascomment{stopped here.}

\subsection{Ideas and Techniques}
\begin{itemize}
\item Global phase+local phase.
\item Reduce from private type, communication allowed to public types.
\item Information theory.
\end{itemize}

