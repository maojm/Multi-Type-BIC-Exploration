\section{Model and Preliminaries}
\label{sec:model}
\jmcomment{Reviewer 2: The model description is not clear. In particular, it did not describe what the principal knows about the states, agents' reward functions etc., and what the agents know. Note that it is crucial to be clear about who knows what since the information asymmetry is essential to the model. Is a new state drawn at every round t or is the state of nature the same across all rounds? I think for the model to make sense, it should be the latter case. Then, the questions are: (1) how does the principal's belief about the state of nature evolve? (2) in the benchmark $OPT_{pub}s$ rightly above Section 3, why you need to take expectation over w? Why not just the particular w realized at the beginning of the game?


Then at each single round, what do principal and agents know about S? 
}

\emph{Bayesian exploration} is a game between a principal and $T$ agents who arrive one by one, with agent $t$ arriving in round $t$. Each agent $t$ has a type $\theta_t\in\varTheta$, drawn independently from a fixed distribution, and an action space $\A$.  There is uncertainty, captured by a latent ``state of nature" $\omega\in \varOmega$, henceforth simply the \emph{state}, drawn from a Bayesian prior at the beginning of time and fixed across rounds. The reward $r_t = u(\theta_t,a_t,\omega)$ is determined by the type $\theta_t$, the action $a_t$, and the state $\omega$, for some fixed, deterministic function
$u:\varTheta\times \A \times \varOmega \to [0,1]$.  We assume that the sets $\mA$, $\varTheta$ and $\varOmega$ are finite. We use $\omega_0$ as the random variable for the state, and write $\Pr[\omega]$ for $\Pr[\omega_0=\omega]$. Similarly, we write $\Pr[\theta]$ for $\Pr[\theta_t=\theta]$. {\bf NSI: do we assume $r$ is bounded or anything like that?} 
\jmcomment{Reward is defined as the utility which is defined to be in $[0,1]$.}
 An {\em instance} of the Bayesian exploration game consists of $T$, the type distribution, the action space, the prior over the state, and the reward function.

A solution to the Bayesian exploration game is a randomized online algorithm $\pi$ termed ``recommendation policy" which, at each round $t$, maps the instance, as defined above, and current history, defined below, to a message $m_t$ which, in general, is an arbitrary bit string of length polynomial in the size of the instance.  The {\em history} includes the rewards and agent type information obtained in the past rounds and current round up until the recommendation is issued.{\bf NSI: is the following footnote required?}\footnote{Formally, for randomized algorithms, we also assume the history contains the random seed and so can simulate the algorithm on prior rounds given the history.}  We consider three model variants, depending on whether and when the principal learns the agent's type: the type is revealed immediately after the agent arrives (\emph{public types}), the type is revealed only after the principal issues a recommendation (\emph{reported types}), the type is not revealed (\emph{private types}). Hence the history at round $t$ is $\{(r_1,\theta_1),\ldots,(r_{t-1},\theta_{t-1}),r_t\}$ for public types, $\{(r_1,\theta_1),\ldots,(r_{t-1},\theta_{t-1})\}$ for reported types, and $\{r_1,\ldots,r_{t-1}\}$ for private types.

The goal of each agent $t$ is to choose an action $a_t$ so as to maximize her Bayesian-expected reward given the instance, her type $\theta_t$, and the policy $\pi$. The goal of the principal is to choose a policy $\pi$ that maximizes (Bayesian-expected) total reward, \ie $\E[\sum_{t=1}^T r_t]$, where the expectation is over the randomness in the instance and the policy.\footnote{Note, the principal must commit to the policy, given only the instance.  The policy, however, includes the history and thus can adapt recommendations to inferences about the state based on the history.  See Example~\ref{exp:simple}.}

\iffalse
\emph{Bayesian Exploration} is a game between a principal and $T$ agents who arrive one by one. Each round $t$ proceeds as follows: a new agent $t$ arrives, receives a message $m_t$ from the principal, chooses an action $a_t$ from a fixed action space $\A$, and collects a reward $r_t\in [0,1]$ that is immediately observed by the principal. Each agent has a type $\theta_t\in\varTheta$, drawn independently from a fixed distribution. There is uncertainty, captured by a latent ``state of nature" $\omega\in \varOmega$, henceforth simply the \emph{state}, drawn from a Bayesian prior. The reward $r_t = u(\theta_t,a_t,\omega)$ is determined by the triple $(\theta_t,a_t,\omega)$, for some fixed, deterministic function
    $u:\varTheta\times \A \times \varOmega \to [0,1]$.
The messages $m_t$ are generated according to a randomized online algorithm $\pi$ termed ``recommendation policy".

The type distribution, the Bayesian prior, and the recommendation policy are common knowledge. Each agent $t$ knows her own type $\theta_t$, and observes nothing else except the message $m_t$.  We consider three model variants, depending on whether and when the principal learns the agent's type: the type is revealed immediately after the agent arrives (\emph{public types}), the type is revealed only after the principal issues a recommendation (\emph{reported types}), the type is not revealed (\emph{private types}). Each agent $t$ chooses action $a_t$ so as to maximize her Bayesian-expected utility given what she knows. The goal of the principal is to maximize (Bayesian-expected) total reward, \ie $\E[\sum_{t=1}^T r_t]$.

We assume that the sets $\mA$, $\varTheta$ and $\varOmega$ are finite. We use $\omega_0$ as the random variable for the state, and write $\Pr[\omega]$ for $\Pr[\omega_0=\omega]$. Similarly, we write $\Pr[\theta]$ for $\Pr[\theta_t=\theta]$.
\fi

\xhdr{Bayesian-incentive compatibility.}
For public types, we assume that the message in each round is an action, \ie $m_t \in \A$, and the recommendation policy $\pi$ is {\em Bayesian incentive-compatible} (\emph{BIC}). This is without loss of generality, by a suitable version of Myerson's ``revelation principle".

The BIC condition is stated as follows. Let $\EE_{t-1}$ be the event that the agents have followed principal's recommendations up to (but not including) round $t$. For any type $\theta$ and round $t$, let $\pi^t(\theta)$ be the action recommended by $\pi$ in round $t$. The recommendation policy is BIC if for each round $t$, type $\theta$, and and any two actions $a,a'$ such that $\Pr[\pi^t(\theta) = a|\EE_{t-1}] > 0$ it holds that
\begin{align}\label{eq:model-BIC-actions}
\E\left[\; u(\theta,a,\omega) - u(\theta,a',\omega) \mid \pi^t(\theta) =a, \EE_{t-1}\;\right] \geq 0.
\end{align}
(The expectation is over the realized state $\omega$ and the randomness in the policy, thus capturing the uncertainty in the agent's information.)

For reported types and private types, we restrict each message to be a \emph{menu}, i.e., a mapping from types to actions, and assume that the recommendation policy satisfies a similar but technically different BIC condition. Again, this is without loss of generality by revelation principle. To state the BIC condition, let $\pi^t$ be the menu recommended in round $t$. The recommendation policy $\pi$ is BIC if for each round $t$, type $\theta$, and any two menus $m,m'$ such that
    $\Pr[\pi^t= m|\EE_{t-1}] > 0$, we have
\begin{align}\label{eq:model-BIC-menus}
\E\left[\; u(\theta,m(\theta),\omega) - u(\theta,m'(\theta),\omega)
    \mid \pi^t = m, \EE_{t-1}\;\right] \geq 0.
\end{align}
(Again, the expectation is over the realized state $\omega$ and the randomness in the policy.)
\vspace{-1mm}
\xhdr{Explorability and benchmarks.}
%For these definitions, recommendation policies proceed indefinitely. 
For public types, a type-action pair $(\theta,a)\in \Theta\times \A$ is called \emph{eventually-explorable} in state $\omega$ if there is some BIC recommendation policy that, for $T$ large enough, eventually recommends this action to this agent type with positive probability. Then action $a$ is called \emph{eventually-explorable} for type $\theta$ and state $\omega$. The set of all such actions is denoted $\AExp_{\omega,\theta}$. Likewise, for private types, a menu is called \emph{eventually-explorable} in state $\omega$ if there is some BIC recommendation policy that eventually recommends this menu with positive probability. The set of all such menus is denoted $\MExp_{\omega}$.

Our benchmark is the best eventually-explorable recommendation:
\begin{align*}
\OPTpub &= \sum_{\theta \in \varTheta, \omega\in \varOmega} \Pr[\omega] \cdot \Pr[\theta] \cdot \max_{a \in \AExp_{\omega,\theta}} u(\theta, a, \omega).\\
    &\qquad\text{(for public types: actions)}\\
\OPTpri &= \sum_{\omega\in \varOmega} \Pr[\omega] \cdot\max_{m \in \MExp_{\omega}}\sum_{\theta \in \varTheta} \Pr[\theta] \cdot  u(\theta, m(\theta), \omega)\\
&\qquad\text{(for private types: menus)}.
\end{align*}
We have $\OPTpub \geq \OPTpri$, essentially because any BIC policy for private types can be simulated as a BIC policy for public types. We provide an example (Example \ref{exp:simple}) when $\OPTpub > \OPTpri$.

\OMIT{
Note that, for all settings, no BIC recommendation policy can out-perform the corresponding benchmark.  Our main technical contributions are (computationally efficient) policies that get arbitrarily close to these benchmarks as the number of agents grows.}
