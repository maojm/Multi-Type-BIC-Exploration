%!Tex root = main.tex
\section{Model and Preliminaries}
\label{sec:model}

\OMIT{\jmcomment{Reviewer 2: The model description is not clear. In particular, it did not describe what the principal knows about the states, agents' reward functions etc., and what the agents know. Note that it is crucial to be clear about who knows what since the information asymmetry is essential to the model. Is a new state drawn at every round t or is the state of nature the same across all rounds? I think for the model to make sense, it should be the latter case. Then, the questions are: (1) how does the principal's belief about the state of nature evolve? (2) in the benchmark $OPT_{pub}s$ rightly above Section 3, why you need to take expectation over w? Why not just the particular w realized at the beginning of the game? Then at each single round, what do principal and agents know about S?
}}

\emph{Bayesian Exploration} is a game between a principal and $T$ agents. The game consists of $T$ rounds. Each round $t\in [T]$ proceeds as follows: a new agent $t$ arrives, receives a message $m_t$ from the principal, chooses an action $a_t$ from a fixed action space $\A$, and collects a reward $r_t\in [0,1]$ that is immediately observed by the principal. Each agent $t$ has a {\em type} $\theta_t\in\varTheta$, drawn independently from a fixed distribution $\DT$, and an {\em action space} $\mA$ (same for all agents).  There is uncertainty, captured by a ``state of nature" $\omega\in \varOmega$, henceforth simply the \emph{state}, drawn from a Bayesian prior $\prior$ at the beginning of time and fixed across rounds. The {\em reward} $r_t = u(\theta_t,a_t,\omega)\in[0,1]$ of agent $t$ is determined by its type $\theta_t$ of agent $t$, the action $a_t\in\mA$ chosen this agent, and the state $\omega$, for some fixed and deterministic \emph{reward function}
$u:\varTheta\times \A \times \varOmega \to [0,1]$.  
Principal's messages $m_t$ are generated according to a randomized online algorithm $\pi$ termed ``recommendation policy". 
Thus, an {\em instance}
    %$\I=(T,\mA,\varTheta,\varOmega,\DT,\prior,u)$
of Bayesian Exploration consists of the time horizon $T$, the sets $\mA,\varTheta,\varOmega$,
the type distribution $\DT$, the prior $\prior$, and the reward function $u$.


The knowledge structure is as follows. The type distribution $\DT$, the Bayesian prior $\prior$, the reward function $u$, and the recommendation policy are common knowledge. Each agent $t$ knows her own type $\theta_t$, and observes nothing else except the message $m_t$.  We consider three model variants, depending on whether and when the principal learns the agent's type: the type is revealed immediately after the agent arrives (\emph{public types}), the type is revealed only after the principal issues a recommendation (\emph{reported types}), the type is not revealed (\emph{private types}).


%{\bf NSI: do we assume $r_t$ is bounded or anything like that?}
%\jmcomment{Reward is defined as the utility which is defined to be in $[0,1]$.}

%The Bayesian Exploration game proceeds sequentially in rounds $t=1,\ldots, T$.  

Let $H_t$ denote the \emph{history} observed by the principal at round $t$, immediately before it chooses its message $m_t$. Hence, it equals $\{(r_1,\theta_1),\ldots,(r_{t-1},\theta_{t-1}),r_t\}$ for public types, $\{(r_1,\theta_1),\ldots,(r_{t-1},\theta_{t-1})\}$ for reported types, and $\{r_1,\ldots,r_{t-1}\}$ for private types.%
\footnote{For randomized policies, the history also contains policy's random seed in each round.}
Formally, this is the input to the recommendation policy in each round $t$. Borrowing terminology from the Bayesian Persuasion literature, we will often refer to the history as the {\em signal}. We denote the set of all possible histories (signals) at time $t$ by $\mH_t$.

\OMIT{A solution to an instance $\I$ of the Bayesian Exploration game is a randomized online algorithm $\pi$ termed ``recommendation policy" which, at each round $t$, maps %instance $\I$ and
the current history $H_t$ to a distribution over messages $m_t$ which, in general, are arbitrary bit strings of length polynomial in the size of the instance.}

The recommendation policy $\pi$, the type distribution $\DT$, the state distribution $\prior$, and the reward function $u$ induce a joint distribution $\DT(\varOmega,\mH_t)$ over states and histories, henceforth called the {\em signal structure} at round $t$. Note that it is known to agent $t$.

We are ready to state agents' decision model. Each agent $t$, given the message $m_t$, chooses an action $a_t$ so as to maximize her {\em Bayesian-expected reward}
\begin{align}
 \E[r_t]\equiv
    \E_{(\omega,H_t)\sim\DT(\varOmega,\mH_t)}
    \left[ 
        u(\theta_t,a_t,\omega)\; \mid m_t\sim\pi(H_t)
    \right].
\end{align}
%$$\E[r_t]\equiv\E_{(\omega,H_t)\sim\DT(\varOmega,\mH_t)}
%[\; \E_{m_t\sim\pi(H_t)}[u(\theta_t,a_t,\omega)\; ]].$$

Given the instance of Bayesian Exploration, the goal of the principal is to choose a policy $\pi$ that maximizes (Bayesian-expected) {\em total} reward, \ie $\sum_{t=1}^T \E[r_t]$. 
\footnote{While the principal must commit to the policy given only the problem instance, the policy itself observes the history and thus can adapt recommendations to inferences about the state based on the history.  See Example~\ref{exp:simple}.}

We assume that the sets $\mA$, $\varTheta$ and $\varOmega$ are finite. We use $\omega_0$ as the random variable for the state, and write $\Pr[\omega]$ for $\Pr[\omega_0=\omega]$. Similarly, we write $\Pr[\theta]$ for $\Pr[\theta_t=\theta]$.

\xhdr{Bayesian-incentive compatibility.}
For public types, we assume the message $m_t$ in each round is a recommended action $a \in \mA$ which, for convenience, we sometimes write as $m_t(\theta_t)$. For private and reported types, we assume that the message $m_t$ in each round is a \emph{menu} mapping types to actions, i.e., $m_t:\varTheta\rightarrow\mA$.  
%In either case, we can write the {\em recommended action} for agent $t$ as $m_t(\theta_t)$. 
 We further assume $\pi$ is Bayesian incentive-compatible. 
\begin{definition}
Let $\EE_t$ be the event that the agents have followed principal's recommendations before round $t$, \ie $a_s = m_s(\theta_s)$ for all rounds $s<t$.
The recommendation policy $\pi$ is {\em Bayesian incentive compatible} (\emph{BIC}) if for all rounds $t$ and messages $m$ such that
\[ \Pr_{(\omega,H_t)\sim\DT(\varOmega,\mH_t)}
    [m = \pi(H_t)\; \mid\; \EE_{t}] > 0,
\]
it holds that for all types $\theta$ and arms $a$,
\begin{align}\label{eq:model-BIC-actions}
\E\left[\; u(\theta,m(\theta),\omega) - u(\theta,a,\omega) \; \mid\; m_t=m, \EE_{t}\;\right] \geq 0,
\end{align}
where the expectation is over $(\omega,H_t)\sim\DT(\varOmega,\mH_t)$.
\end{definition}

\noindent The above assumptions are without loss of generality, by a suitable version of Myerson's ``revelation principle".
%(see {\bf NSI: citation} for more details).
% A.S. no citations to give! We have smth in the ec16 paper, 
% but it is not quite the same. We'll need to write it out eventually!

\iffalse
For reported types and private types, we assume that the message in each round is a \emph{menu} a mapping types to actions, and assume that the recommendation policy satisfies a similar but technically different BIC condition. Again, this is without loss of generality by revelation principle. to state the BIC condition, let $\pi^t$ be the menu recommended in round $t$. The recommendation policy $\pi$ is BIC if for each round $t$, type $\theta$, and any two menus $m,m'$ such that
    $\Pr[\pi^t= m|\EE_{t-1}] > 0$, we have
\begin{align}\label{eq:model-BIC-menus}
\E\left[\; u(\theta,m(\theta),\omega) - u(\theta,m'(\theta),\omega)
    \mid \pi^t = m, \EE_{t-1}\;\right] \geq 0.
\end{align}
(Again, the expectation is over the realized state $\omega$ and the randomness in the policy.)
\fi

\xhdr{Explorability and benchmarks.}
%For these definitions, recommendation policies proceed indefinitely.
For public types, a type-action pair $(\theta,a)\in \Theta\times \A$ is called \emph{eventually-explorable} in state $\omega$ if there is some BIC recommendation policy that, for $T$ large enough, eventually recommends this action to this agent type with positive probability. Then action $a$ is called \emph{eventually-explorable} for type $\theta$ and state $\omega$. The set of all such actions is denoted $\AExp_{\omega,\theta}$. 

Likewise, for private types, a menu is called \emph{eventually-explorable} in state $\omega$ if there is some BIC recommendation policy that eventually recommends this menu with positive probability. The set of all such menus is denoted $\MExp_{\omega}$.

Our benchmark is the best eventually-explorable recommendation for each type. For public and private types, resp., this is
\begin{align}
\OPTpub &= \sum_{\theta \in \varTheta, \omega\in \varOmega} \Pr[\omega] \cdot \Pr[\theta] \cdot \max_{a \in \AExp_{\omega,\theta}} u(\theta, a, \omega).
    %&\qquad\text{(for public types: actions)}
    \label{eq:bench-public}\\
\OPTpri &= \sum_{\omega\in \varOmega} \Pr[\omega] \cdot\max_{m \in \MExp_{\omega}}\sum_{\theta \in \varTheta} \Pr[\theta] \cdot  u(\theta, m(\theta), \omega).\label{eq:bench-private}
%&\qquad\text{(for private types: menus)}.
\end{align}
We have $\OPTpub \geq \OPTpri$, essentially because any BIC policy for private types can be simulated as a BIC policy for public types. We provide an example (Example \ref{exp:simple}) when $\OPTpub > \OPTpri$.

\OMIT{
Note that, for all settings, no BIC recommendation policy can out-perform the corresponding benchmark.  Our main technical contributions are (computationally efficient) policies that get arbitrarily close to these benchmarks as the number of agents grows.}
