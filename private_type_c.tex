%!TEX root = main.tex

\subsection{Extension to Reported Types}
\label{sec:private_c}

\newcommand{\pipub}{\pi_{\term{pub}}}

Let us sketch how to extend our ideas for public types to handle the case of reported types. We'd like to simulate the recommendation policy for public types, call it $\pipub$. We simulate it separately for the exploration part and the exploitation part. The exploitation part is fairly easy: we provide a menu that recommends the best explored action for each agent types. In the exploration part, in each round $t$ we guess the agent type to be $\hat{\theta}_t$, with equal probability among all types. The idea is to simulate $\pipub$ only in \emph{lucky rounds} when we guess correctly, \ie $\hat{\theta}_t=\theta_t$. Thus, 
in each round $t$ we simulate the $\ell_t$-th round of $\pipub$, where $\ell_t$ is the number of lucky rounds before round $t$.

In each round $t$ of exploration, we suggest the following menu. For type $\hat{\theta}_t$, we recommend the same action as $\pipub$ would recommend for this type in the $\ell_t$-th round, namely $\pipub^{\ell_t}(\hat{\theta}_t)$.
For any other type, we recommend the action which has the best expected reward given prior knowledge and the action suggested to type $\hat{\theta}$.  We need to  ensure that the suggested actions in the menu are BIC and do not convey extra information to an agent of type $\hat{\theta}$. When we receive the reported type, we can check whether our guess was correct. If so, we input the type-action-reward triple back to $\pipub$. Else, we ignore this round, as if it never happened.

Thus, our recommendation policy eventually explores the same type-action pairs as $\pipub$. The expected number of rounds increases by the factor of $|\varTheta|$. Thus, we have the following theorem.

\begin{theorem}
\label{thm:reported}
Consider Bayesian Exploration with reported types.
There exists a BIC recommendation policy whose expected total reward is at least $\left(T - C \right) \cdot \OPTpub$, 
for some constant $C$ that depends on the problem instance but not on $T$.
This policy explores all type-action pairs that are eventually-explorable for a given state in the case of public types.
\end{theorem}

%$C = |\A| \cdot |\varTheta| \cdot \sum_{\theta\in\varTheta} \frac{L_{\theta}\cdot |\varTheta|}{\Pr[\theta]}$.
