%!TEX root = main.tex

\section{Bayesian Exploration with Reported Types}
\label{sec:private_c}
In this section, we show a BIC recommendation policy for reported types. The main idea is to simulate the recommendation policy for public types in Section \ref{sec:public}, let's call it $\pi^{pub}$. We describe how we simulate it separately for the exploration part and the exploitation part.

In the exploration part, our recommendation policy guesses the type with equal probability among all types in each round. The plan is to simulate $\pi^{pub}$ only in rounds when we guess types correctly. In each round, after we guess the type to be $\hat{\theta}$, we suggest the following menu to agents.
\begin{itemize}
\item For type $\hat{\theta}$, we suggest the action that $\pi^{pub}$ would suggest if the agent has type $\hat{\theta}$.
\item For some other type which is not $\hat{\theta}$, we suggest the action which has the best expected reward given prior knowledge and the action suggested to type $\hat{\theta}$.  We just need to make sure that the suggested actions in the menu are BIC and do not convey extra information to an agent of type $\hat{\theta}$.
\end{itemize}
In the end of the round, we get the reported type. If our guess is correct, we input the type-action-reward triple back to $\pi^{pub}$. If our guess is wrong, we totally ignore what we learn in this round. By doing this simulation, similarly as $\pi^{pub}$, our recommendation policy explores all actions in $\A^{exp}_{\omega,\theta}$ for any $\theta \in \varTheta$ when the state is $\omega$. Such simulation makes the exploration part to have more rounds as we guess types correctly with probability $\frac{1}{|\varTheta|}$. The expected number of rounds of the exploration part has an extra factor $|\varTheta|$: $|\A| \cdot |\varTheta| \cdot \sum_{\theta\in\varTheta} \frac{L_{\theta}\cdot |\varTheta|}{\Pr[\theta]}$.

The exploitation part is much easier to simulate as we don't need to know types to do exploitation. We just provide a menu consists of the best explored actions of all types. This will have the same performance as the exploitation part in $\pi^{pub}$.

To conclude, we have the following theorem.

\begin{theorem}
\label{thm:reported}
We have a BIC recommendation policy of $T$ rounds with expected total reward at least $\left(T - C \right) \cdot \OPT$ for some constant $C = |\A| \cdot |\varTheta| \cdot \sum_{\theta\in\varTheta} \frac{L_{\theta}\cdot |\varTheta|}{\Pr[\theta]}$. When the state is $\omega$, it explores all the actions in $\A^{exp}_{\omega,\theta}$ for any type $\theta \in \varTheta$.
\end{theorem}
