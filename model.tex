\section{Model and Preliminaries}

\emph{Bayesian Exploration} is a game between a principal and $T$ agents who arrive one by one. Each round $t$ proceeds as follows: a new agent $t$ arrives, receives a signal $s_t$ from the principal, chooses an action $a_t$ from a fixed and finite action space $\A$, and collects a reward $r_t\in [0,1]$ that is immediately observed by the principal. Each agent has a type $\theta_t\in\varTheta$, drawn independently from a fixed distribution $\DT$. There is uncertainty, captured by a latent ``state of nature" $\omega\in \varOmega$, henceforth simply the \emph{state}, drawn from a Bayesian prior. The reward $r_t = u(\theta_t,a_t,\omega)$ is determined by the triple $(\theta_t,a_t,\omega)$, for some fixed, deterministic function
    $u:\varTheta\times \A \times \varOmega \to [0,1]$.
The signals $s_t$ are generated according to a randomized online algorithm $\pi$ termed ``recommendation policy".

The knowledge structure is as follows. The type distribution $\DT$, the Bayesian prior, and the recommendation policy are common knowledge. Each agent $t$ knows her own type $\theta_t$, and observes nothing else except the signal $s_t$.  We consider three model variants, depending on whether and when the principal learns the agent's type: the type is revealed immediately after the agent arrives (\emph{public types}), the type is revealed only after the principal issues a recommendation (\emph{reported types}), the type is not revealed (\emph{private types}).


Each agent $t$ makes its choice $a_t$ so as to maximize its Bayesian-expected utility given what she knows: the Bayesian prior, the type distribution $\DT$, the recommendation policy $\pi$, and the signal $s_t$. Principal's goal is to maximize (Bayesian-expected) social welfare, \ie $\E[\sum_{t=1}^T r_t]$. 

We use $\Omega$ as the random variable for the state, and write $\Pr[\omega]$ for $\Pr[\Omega=\omega]$. {\bf NSI: do we need to say this? we don't anymore say the similar thing about $\theta$.  i favor deleting this paragraph.}

\xhdr{Bayesian-incentive compatibility.}
For public types, we assume that the signal in each round is an action, \ie $s_t \in \A$, and the recommendation policy $\pi$ is {\em Bayesian incentive-compatible} (\emph{BIC}). This is without loss of generality, by a suitable version of Myerson's ``revelation principle".

The BIC condition is stated as follows. Let $\EE_{t-1}$ be the event that the agents have followed principal's recommendations up to (but not including) round $t$. For any type $\theta$ and round $t$, let $\pi^t(\theta)$ be the action recommended by $\pi$ in round $t$. The recommendation policy is BIC if for each round $t$, type $\theta$, and and any two actions $a,a'$ such that $\Pr[\pi^t(\theta) = a|\EE_{t-1}] > 0$ it holds that
\[
\E\left[\; u(\theta,a,\omega) - u(\theta,a',\omega) \mid \pi^t(\theta) =a, \EE_{t-1}\;\right] \geq 0.
\]
(The expectation is over the realized state $\omega$ and the random seed of the algorithm.)

For private types, we restrict each signal to be a \emph{menu}: a mapping from types to actions, and assume that the recommendation policy satisfies a similar but technically different BIC condition. Again, this is without loss of generality by revelation principle. To state the BIC condition, let $\pi^t$ be the menu recommended in round $t$. The recommendation policy $\pi$ is BIC if for each rounds $t$, type $\theta$, and any two menus $m,m'$ such that 
    $\Pr[\pi^t= m|\EE_{t-1}] > 0$, we have
\[
\E\left[\; u(\theta,m(\theta),\omega) - u(\theta,m'(\theta),\omega) 
    \mid \pi^t = m, \EE_{t-1}\;\right] \geq 0.
\]
 
\xhdr{Explorability.}
For the sake of these definitions, let us assume that recommendation policies proceed indefinitely, \ie without a particular time horizon. 

For public types, a type-action pair $(\theta,a)\in \Theta\times \A$ is called \emph{eventually-explorable} in state $\omega$ if there is some BIC recommendation policy that eventually recommends this action to an agent of this type with positive probability (over the random seed of the policy). We also say that action $a$ is eventually-explorable for type $\theta$ and state $\omega$. The set of all such states is denoted $\A_{\omega,\theta}^{exp}$.


Likewise, for private types, a menu is called \emph{eventually-explorable} in state $\omega$ if there is some BIC recommendation policy that eventually recommends this menu with positive probability (over the random seed of the policy). The set of all such menus is denoted $\M_{\omega}^{exp}$. {\bf NSI: I'm in favor of removing the $exp$ from these expressions.  Feels redundant, given the subscripts.}

\xhdr{Benchmarks.}
We measure the success of the principal with respect to the expected utility of the best eventually-explorable recommendation:
\begin{align*}
\OPT_1 &= \sum_{\theta \in \varTheta, \omega\in \varOmega} \Pr[\omega] \cdot \Pr[\theta] \cdot \max_{a \in \A_{\omega,\theta}^{exp}} u(\theta, a, \omega).
    &\qquad\text{(for public types)}\\
\OPT_2 &= \sum_{\omega\in \varOmega} \Pr[\omega] \cdot\max_{m \in \M_{\omega}^{exp}}\sum_{\theta \in \varTheta} \Pr[\theta] \cdot  u(\theta, m(\theta), \omega)
&\qquad\text{(for private types)}.
\end{align*}

\OMIT{
Note that, for all settings, no BIC recommendation policy can out-perform the corresponding benchmark.  Our main technical contributions are (computationally efficient) policies that get arbitrarily close to these benchmarks as the number of agents grows.}
