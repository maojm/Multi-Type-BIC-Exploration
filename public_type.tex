%!TEX root = main.tex

\section{Bayesian Exploration with Public Types}
\label{sec:public}
In this section, we show a BIC scheme for public types (as in Theorem \ref{thm:public}). We formally state Theorem \ref{thm:public} in Section \ref{sec:public_bench} and we prove it in Section \ref{sec:public_single}, \ref{sec:public_maxe} and \ref{sec:public_main}.

\subsection{Explorability and Benchmark}
\label{sec:public_bench}
In this subsection, we define the benchmark and state our main theorem (Theorem \ref{thm:public}). 

\begin{definition}
\label{def:public_exp}
An action $a$ of type $\theta$ is eventually-explorable, for a given state $\omega$, if there exists a BIC recommendation policy $\pi$ and some round $t$ such that $\Pr[\pi^t(\theta)= a]> 0$. The set of all such actions for state $\omega$ and type $\theta$ is denoted as $\A_{\omega,\theta}^{exp}$.
\end{definition}

\begin{definition}[Benchmark]
Define benchmark as 
\[
\OPT = \sum_{\theta \in \varTheta, \omega\in \varOmega} \Pr[\omega] \cdot \Pr[\theta] \cdot \max_{a \in \A_{\omega,\theta}^{exp}} u(\theta, a, \omega).
\]
\end{definition}

Notice that for a given state $\omega$ and type $\theta$, any BIC recommendation policy can only recommend an action in $\A_{\omega,\theta}^{exp}$. We can simply get the following claim which says that no BIC recommendation policy can get expected per round reward better than the benchmark:
\begin{claim}
Any BIC recommendation policy of $T$ rounds has expected total reward at most $T \cdot\OPT$.  
\end{claim}

On the other hand, we construct a BIC recommendation policy that nearly achives the benchmark. It's proved in the following subsections.
\begin{theorem}
\label{thm:public}
We have a BIC recommendation policy of $T$ rounds with expected total reward at least $\left(T - C \right) \cdot \OPT$ for some constant $C$ which does not depend on $T$. 
\end{theorem}

\subsection{Single-round Exploration}
\label{sec:public_single}
%This section is very similar to the EC16's paper, we include this section to make our paper self-contained. 

In this subsection, we consider a single round of the Bayesian exploration. As we only consider one round, we fix the agent's type in this round to be $\theta$.

A signal structure $\S$ can be specified by the signal support $\X$ and a joint distribution on $(\X, \varOmega)$.  %Do we actually need signal structures?
\begin{definition}
Consider a single-round of Bayesian exploration when the principal has signal $S$ from signal strcture $\S$. An action $a \in \A$ is called signal-explorable, for a given signal $s$, if there exists a single-round BIC recommendation policy $\pi$ such that $\Pr[\pi(s) = a] > 0$. The set of all such actions is denoted as $\EX_s[\S]$. The signal-explorable set is defined as $\EX[\S] = \EX_S[\S]$. 
\end{definition}

Note that $\EX_s[\S]$ is a fixed subset and $\EX[\S]$ is a random variable whose realization is determined by the realization of signal $S$. 

\begin{definition}
We say random variable $S$ is at least as informative as  random variable $S'$ about state $\Omega$ if $I(S' ; \Omega|S) = 0$. 
\end{definition}

\begin{lemma}
\label{lem:infomono}
Let $S$ and $S'$ be two random variables and $\S$ and $\S'$ be their signal structures. If $S$ is at least as informative as $S'$ about state $\Omega$ (i.e. $I(S' ; \Omega|S) = 0$), then $\EX_{s'}[\S'] \subseteq \EX_s[\S]$ for all $s' ,s$ such that $\Pr[S= s, S'= s'] > 0$. 
\end{lemma}

\begin{proof}
Consider any BIC scheme $\pi'$ for signal structure $\S'$. We construct $\pi$ for signature structure $\S$ by setting $\Pr[\pi(s) = a] = \sum_{s'} \Pr[\pi'(s') = a] \cdot Pr[S' = s'|S = s]$. It's easy to check $\pi$ is BIC. For any $s', s ,a$ such that $Pr[S' = s',S = s] >0 $ and $\Pr[\pi'(s') = a] >0$, we have $\Pr[\pi(s) = a] > 0$. This implies $\EX_{s'}[\S'] \subseteq \EX_s[\S]$. 
\end{proof}

For a given signal $s \in \X$ and an action $a \in \A$, we solve the following LP to check if $a$ is signal-exlporable given signal $s$:

\begin{figure}[H]
\begin{mdframed}
\begin{alignat*}{2}
  \textbf{maximize }   & x_{a,s}\  \\
  \textbf{subject to: } & \sum_{\omega \in \varOmega, s' \in \X} \Pr[\omega] \cdot \Pr[s' | \omega] \cdot \left(u(\theta, a', \omega) - u(\theta, a'', \omega)\right) \cdot x_{a',s'} \geq 0   &\ & \forall a',a'' \in \A \\
                       & \sum_{a'\in \A} x_{a',s'} = 1,  \ &\ & \forall s' \in \X \\
                       & x_{a',s'} \geq 0,  \ &\ & \forall s' \in \X, a'\in \A \\
\end{alignat*}
\end{mdframed}
%\caption{LP}
\label{fig:public_lp}
\end{figure}

Notice that the constraints in the above LP characterize any BIC recommendation scheme. Therefore we get the following claim. 
\begin{claim}
For a given signal $s \in \X$, an action $a \in \A$ is signal-explorable if and only if the above LP has a postive solution. When the LP has a positive solution, define $\pi^{a,s}$ as $\Pr[\pi^{a,s}(s') = a'] = x_{a',s', } \forall a' \in \A, s' \in \X$. Then $\pi^{a,s}$ is a single-round BIC recommendation policy such that $\Pr[\pi^{a,s}(s) = a] > 0$
\end{claim}

\begin{definition}[Max-support policy]
Given a signal structure $\S$, a BIC recommendation policy $\pi$ is called the max-support policy if $\forall s \in \X$  and signal-explorable action $a\in \A$ given $s$, $\Pr[\pi(s) = a] > 0$. 
\end{definition}


We can get a max-support policy $\pi^{max}$ by averaging over BIC recommendation policies for signal explorable actions.
\begin{claim}
\label{clm:pimax}
The following $\pi^{max}$ is a BIC and max-support policy. 
\[
\pi^{max} = \frac{1}{|\X|} \sum_{s \in \X} \frac{1}{|\EX_s[\S]|} \sum_{a \in \EX_s[S]} \pi^{a,s}.
\]
\end{claim}

\subsection{MaxExplore}
\label{sec:public_maxe}
%This section is very similar to the EC16's paper, we include this section to make our paper self-contained. 

In this subsection, we show a subroutine MaxExplore which converts $\pi^{max}$ into a sequence of actions. We are going to assume $L_{\theta} \geq \max_{a,s: \Pr[\pi^{max}(s)=a] \neq 0} \frac{1}{\Pr[\pi^{max}(s)=a]}$. 
 \begin{algorithm}[H]
    \caption{Subroutine MaxExplore}
    	\label{alg:public_explore}
    \begin{algorithmic}[1]
	\STATE \textbf{Input:} type $\theta$, signal $S$ and signal strcuture $\S$.
	\STATE \textbf{Output:} a list of actions $\alpha$
	\STATE Compute $\pi^{max}$ as stated in Claim \ref{clm:pimax}.
	%\IF {$l \leq |\A| \cdot |\varTheta|$}
		\STATE Initialize $Res = L_{\theta}$.
		\FOR {each action $ a \in \A$}
                     		\STATE Add $\lfloor L_{\theta} \cdot \Pr[\pi^{max}(S) = a]\rfloor$ copies of action $a$ into list $\alpha$.
			\STATE $Res \leftarrow Res -\lfloor L_{\theta} \cdot \Pr[\pi^{max}(S) = a] \rfloor $.
			\STATE $p^{Res}(a)\leftarrow  L_{\theta} \cdot \Pr[\pi^{max}(S) = a] -  \lfloor L_{\theta} \cdot \Pr[\pi^{max}(S) = a]\rfloor$
		\ENDFOR 
		\STATE $p^{Res}(a) \leftarrow p^{Res}(a) / Res$, $\forall a \in \A$. 
		\STATE Sample $Res$ many actions from distribution according to $p^{Res}$ independently and add these actions into $\alpha$. 
		\STATE Randomly permute the actions in $\alpha$.
	%\ELSE
		%\STATE Add $L_{\theta}$ copies of the best explored action of type $\theta$ according to signal $S$ into action list $\alpha$. 
	%\ENDIF
	\RETURN $\alpha$.	 
     \end{algorithmic}
\end{algorithm}

Such conversion is exactly the same as \cite{EC16 paper}, we have the following two claims. 

\begin{claim}
\label{clm:maxexplore}
Given type $\theta$ and signal $S$, MaxExplore explores all the signal-explorable actions.  
\end{claim}

\begin{claim}
Signals and signal structures can be efficiently computed and represented.
\end{claim}

\subsection{Main Scheme}
\label{sec:public_main}
In this subsection, we show our main scheme which explores all the eventually-explorable actions and then advises the agents to choose the best actions among eventually-explorable actions.

Algorithm \ref{alg:public_main} is the main procedure of our scheme. It globally divides rounds into epochs and runs a separate thread for each type (Sub-$\theta$ for type $\theta$). We pick $L_{\theta}$ to be at least $\max_{a,s: \Pr[\pi(s)=a] \neq 0} \frac{1}{\Pr[\pi(s)=a]}$ for all $\pi$ that might be chosen as $\pi^{max}$ in Algorithm \ref{alg:public_main}.
 \begin{algorithm}[H]
    \caption{Main procedure for public types }
    	\label{alg:public_main}
    \begin{algorithmic}[1]
    	\STATE Initial signal $S_1 = \S_1 = \perp$.
	\STATE Initial epoch count $l = 1$. 
	\FOR {$t=1$ to $T$}
		\STATE Let $\theta_t$ to be the agent type of current round $t$. Run subroutine Sub-$\theta_t$.
		\IF {every type has finished a complete phase in the current epoch}
			\STATE Start a new epoch:
			\STATE $l \leftarrow l + 1$.
			\STATE Let $S_l$ be the set of type-action-reward triples and $\S_l$ be the signal structure when fixing $\theta_1,...,\theta_t$.
		\ENDIF
	\ENDFOR
     \end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:public_sub} describes the thread for type $\theta$. It is activated whenever the agent in the current round has type $\theta$. It divides the rounds of type $\theta$ into phases of fixed length ($L_{\theta}$). Each phase only uses the information collected before the current epoch starts. 

 \begin{algorithm}[H]
    \caption{Subroutine for type $\theta$: Sub-$\theta$ }
    	\label{alg:public_sub}
    \begin{algorithmic}[1]
	\FOR {each call from Algorithm \ref{alg:public_main}}
		\IF {previous phase has finished ($i_{\theta} > L_{\theta}$) or this is the first call}
			\STATE Start a new phase:
			\IF {$l \leq |\A| \cdot |\varTheta|$}
				\STATE Use the current $S_l$ and $\S_l$ to compute a list of $L_{\theta}$ actions $\alpha_{\theta} \leftarrow $ MaxExplore($\theta, S_l, \S_l$).
			\ELSE
				\STATE Add $L_{\theta}$ copies of the best explored action of type $\theta$ according to signal $S$ into action list $\alpha_{\theta}$. 
			\ENDIF
			\STATE Intialize the index of current phase $i_{\theta} \leftarrow 1$.
		\ENDIF
		\STATE Suggest action $\alpha_{\theta} [i_{\theta}]$ to the agent.
		\STATE $i_{\theta} \leftarrow i_{\theta} + 1$.
	\ENDFOR
     \end{algorithmic}
\end{algorithm}


The proof plan is to first give an upper bound on the expected number of rounds of an epoch in Lemma \ref{lem:epoch} and then show in Lemma \ref{lem:exp_public} that Algorithm \ref{alg:public_main} explores all the explorable type-action pair in $|\A| \cdot |\varTheta|$ epochs. We use these two lemmas to prove our main theorem (Theorem \ref{thm:public}) in Corollary \ref{cor:public}. 

First of all, it's easy to check that for each agent, it is Bayesian incentive compatible to follow the recommended action if previous agents all follow the recommended actions. Therefore we have the following claim. 
\begin{claim}
\label{clm:public_BIC}
The scheme in this section is BIC.
\end{claim}

\begin{lemma}
\label{lem:epoch}
For any $l>0$, the expected number of rounds of the first $l$ epochs is at most $l \cdot \sum_{\theta\in\varTheta} \frac{2L_{\theta}}{\Pr[\theta]}$. 
\end{lemma}

\begin{proof}
First of all, if a type $\theta$ shows up $2L_{\theta}$ times, type $\theta$ must have finished a complete phase in these rounds. Therefore, the expected number of rounds of an epoch is at most the expected number of rounds in which each type $\theta$ has shown up at least $2L_{\theta}$ times. The latter expectation can be easily bounded by $\sum_{\theta\in\varTheta} \frac{2L_{\theta}}{\Pr[\theta]}$. Therefore, the expected number of rounds of the first $l$ epochs is at most $l \cdot \sum_{\theta\in\varTheta} \frac{2L_{\theta}}{\Pr[\theta]}$. 
\end{proof}

Notice that in Algorithm \ref{alg:public_main} the division of phases and epochs depends only on realized types $\theta_1,...,\theta_T$ as each phase has a fixed length. We then have the following claim.   
\begin{claim}
Given $\theta_1,...,\theta_T$ as the realized types in $T$ rounds, the division of phases and epochs in Algorithm \ref{alg:public_main} is fixed. 
\end{claim}

\begin{lemma}
\label{lem:exp_public}
For any $l>0$ and a fixed sequence of types $\theta_1,...,\theta_T$ as the types encountered by Algorithm \ref{alg:public_main}, assume Algorithm \ref{alg:public_main} has at least $\min(l, |\A|\cdot |\varTheta|)$ epochs. 
For a given state $\omega$, if an action $a$ of type $\theta$ can be explored by a BIC recommendation policy $\pi$ at round $l$ (i.e. $ \Pr[\pi^l(\theta)= a]> 0$), then such action is guaranteed to be explored by Algorithm \ref{alg:public_main} by the end of epoch $\min(l, |\A|\cdot |\varTheta|)$. 
\end{lemma}

\begin{proof}
We prove this by induction on $l$ for $l \leq |\A|\cdot |\varTheta|$. Base case $l=1$ is trivial by Claim \ref{clm:maxexplore}. Assuming the lemma is correct for $l-1$, let's prove it's correct for $l$. 

Let $S$ be the history of Algorithm \ref{alg:public_main} by the end of epoch $l-1$. By the definition of Algorithm 2, $S = S_{l-1} | \theta_1,...,\theta_T$.  Let $S'$ be the history of $\pi$ in the first $l-1$ rounds. More precisely, $S' = R, H_1,...,H_{l-1}$. Here $R$ is the internal randomness of scheme $\pi$ and $H_t = (\Theta_t, A_t, u(\Theta_t, A_t, \Omega))$ is the triple of type, action and reward in round $t$ of scheme $\pi$. 

First of all, we have
\[
I(S'; \Omega| S) = I(R,H_1,...,H_{l-1}; \Omega| S)  = I(R; \Omega| S) + I(H_1,...,H_{l-1}; \Omega|S, R) = I(H_1,...,H_{l-1}; \Omega|S, R). 
\]

By the chain rule of mutual information, we have
\[
 I(H_1,...,H_{l-1}; \Omega|S, R) = I(H_1;\Omega|S,R) + I(H_2;\Omega|S, R ,H_1) + \cdots + I(H_{l-1}; \Omega|S,R,H_1,...,H_{l-2}). 
\]

For all $t \in [l-1]$, we have
\begin{align*}
&I(H_t; \Omega|S,R,H_1,...,H_{t-1}) \\
=& I(\Theta_t, A_t, u(\Theta_t, A_t, \Omega); \Omega|S,R,H_1,...,H_{t-1}) \\
=& I(\Theta_t ; \Omega|S,R,H_1,...,H_{t-1}) +  I(A_t, u(\Theta_t, A_t, \Omega); \Omega|S,R,H_1,...,H_{t-1},\Theta_t) \\
=& I(A_t, u(\Theta_t, A_t, \Omega); \Omega|S,R,H_1,...,H_{t-1},\Theta_t). \\
\end{align*}
Notice that $A_t$ is a deterministic function of $R,H_1,...,H_{t-1},\Theta_t$ because $A_t$ only depend on the current type $\Theta_t$. Also notice that, by induction hypothesis, $u(\Theta_t, A_t, \Omega)$ is a deterministic function of $|S,R,H_1,...,H_{t-1},\Theta_t, A_t$. Therefore we have
\[
I(H_t; \Omega|S,R,H_1,...,H_{t-1}) = 0, \forall t \in [l-1].
\]
Then we get 
\[
I(S'; \Omega | S) = 0.
\]

By Lemma \ref{lem:infomono}, we know that $\EX[\S'] \subseteq \EX[\S]$. For state $\omega$, there exists a signal $s'$ such that $\Pr[S'=s'|\Omega =\omega] >0 $ and $a \in \EX_{s'} [\S']$. Now let $s$ be the realized value of $S$ given $\Omega = \omega$, we know that $\Pr[S'=s'|S=s] >0$, so $a \in \EX_s[\S]$. By Claim \ref{clm:maxexplore}, we know that a complete phase of type $\theta$ in epoch $l$ of Algorithm \ref{alg:public_main} will choose action $a$ at least once. 

Now we consider the case when $l > |\A| \cdot |\varTheta|$. Define Algorithm $E$ to be the variant of Algorithm \ref{alg:public_main} in the sense that Algorithm $E$ always tries to run MaxExplore (i.e. in Sub-$\theta$, ignores if condition in step 5 and always runs MaxExplore ). For $l > |\A| \cdot |\varTheta|$, the above induction proof still work on Algorithm $E$. Now notice if in some epoch, no unexplored type-action pair is played, Algorithm $E$'s signal won't change and after that Algorithm $E$ won't explore any unexplored type-action pair. As there are $ |\A| \cdot |\varTheta|$ type-action pairs, Algorithm $E$ only explores unexplored type-action pair in the first $ |\A| \cdot |\varTheta|$ epochs. Therefore, the first $\min(l,|\A|\cdot |\varTheta|)$ epochs of Algorithm \ref{alg:public_main} explores the same set of type-action pair as the first $l$ epochs of Algorithm $E$.
\end{proof}

\begin{corollary}[Restatement of Theorem \ref{thm:public}]
\label{cor:public}
We have a BIC recommendation policy (Algorithm \ref{alg:public_main}) of $T$ rounds with expected total reward at least $\left(T - C \right) \cdot \OPT$ for constant $C = |\A| \cdot |\varTheta| \cdot \sum_{\theta\in\varTheta} \frac{2L_{\theta}}{\Pr[\theta]}$. 
\end{corollary}

\begin{proof}
First of all, by Claim \ref{clm:public_BIC}, Algorithm \ref{alg:public_main} is BIC. 

By Lemma \ref{lem:exp_public}, for each state $\omega$, Algorithm \ref{alg:public_main} explores all the eventually explorable actions (i.e. $\A_{\omega,\theta}^{exp}$) for each type $\theta$ by the end of $|\A|\cdot |\varTheta|$ epochs. After that, for each type $\theta$, Algorithm \ref{alg:public_main} will always recommend action $ \arg\max_{a \in \A_{\omega,\theta}^{exp}} u(\theta, a, \omega)$. Therefore Algorithm \ref{alg:public_main} gets reward $OPT$ except rounds in first $|\A|\cdot |\varTheta|$ epochs. By Lemma \ref{lem:epoch}, we know that the expected number of rounds of the first  $|\A|\cdot |\varTheta|$ epochs is at most $C$. Therefore, Algorithm \ref{alg:public_main} has expected total reward at least  $\left(T - C \right) \cdot \OPT$.

\end{proof}