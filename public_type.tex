%!TEX root = main.tex

\section{Bayesian Exploration with Public Types}
\label{sec:public}

In this section, we develop our recommendation policy for public types. Throughout, $\OPT = \OPTpub$.

\begin{theorem}
\label{thm:public}
Consider an arbitrary instance of Bayesian Exploration with public types.
There exists a BIC recommendation policy with expected total reward at least $\left(T - C \right) \cdot \OPT$, for some constant $C$ that depends on the problem instance but not on $T$. This policy explores all type-action pairs that are eventually-explorable for a given state.
\end{theorem}

\subsection{A single round of Bayesian Exploration}
\label{sec:public_single}
%This section is very similar to the EC16's paper, we include this section to make our paper self-contained.

Let us analyze a single round of the Bayesian exploration. Throughout, let $\theta$ be the agent's type.

\xhdr{Signal and explorability.}
We posit that the principal receives a signal $S$ before the round starts, where $S$ is a random variable.  The \emph{signal structure} of $S$, denoted $\S$, consists of the support set $\X$ and a joint distribution of $(S, \omega_0)$. Here $S$ represents the history of the previous rounds and the internal random seed, and the signal structure represents information about $S$ available to the current agent. However, for this subsection it is more lucid to consider an arbitrary signal structure.

A single-round recommendation policy $\pi$ is simply a randomized mapping from signal realizations to actions. The policy is called \emph{Bayesian-incentive compatible} (\emph{BIC}) for signal $S$ if for each type $\theta$ and any two actions $a,a'$ such that $\Pr[\pi(S) = a] > 0$ it holds that
\begin{align}\label{eq:model-BIC-single-round}
\E\left[\; u(\theta,a,\omega) - u(\theta,a',\omega) \mid \pi(S) =a\;\right] \geq 0.
\end{align}

\begin{definition}
Consider a single-round of Bayesian exploration when the principal receives signal $S$ with signal structure $\S$. An action $a \in \A$ is called signal-explorable, for a given type $\theta$ and realized signal $s$, if there exists a single-round BIC recommendation policy $\pi$ such that $\Pr[\pi(s) = a] > 0$. The set of all such actions is denoted as $\EX_s[\S]$. The signal-explorable set is defined as $\EX[\S] = \EX_S[\S]$.
\end{definition}

Note that $\EX[\S]$ is a random subset of actions whose realization is determined by $S$.

\xhdr{Information-monotonicity.}
We compare the information content of two signals using the notion of conditional mutual information (see Appendix~\ref{app:info-theory} for definition and background). Essentially, we show that a more informative signal leads to the same or larger explorable set.

\begin{definition}
We say that signal $S$ \emph{is at least as informative} as signal $S'$ if $I(S' ; \omega_0\mid S) = 0$.
\end{definition}

Intuitively, the condition $I(S';\omega_0|S)= 0$  means if one is given random variable $S$, one can learn no further information from $S'$ about $\omega_0$. Note that this condition depends not only the signal structures of the two signals, but also on their joint distribution.

\begin{lemma}
\label{lem:infomono}
Let $S,S'$ be two signals with signal structures $\S,\S'$. If $S$ is at least as informative as $S'$, then $\EX_{s'}[\S'] \subseteq \EX_s[\S]$ for all $s' ,s$ such that $\Pr[S= s, S'= s'] > 0$.
\end{lemma}

\begin{proof}
Consider any BIC recommendation policy $\pi'$ for signal structure $\S'$. We construct $\pi$ for signal structure $\S$ by setting $\Pr[\pi(s) = a] = \sum_{s'} \Pr[\pi'(s') = a] \cdot Pr[S' = s'\mid S = s]$. Notice that $I(S' ; \omega_0\mid S) = 0$ implies $S'$ and $\omega_0$ are independent given $S$, i.e $\Pr[S' = s'\mid S=s] \cdot \Pr[\omega_0 = \omega\mid S=s] = \Pr[S'=s', \omega_0 = \omega\mid S=s]$ for all $s,s',\omega$. Therefore, for all $s'$ and $\omega$,
\begin{align*}
\textstyle
\sum_s \Pr[S' = s'\mid S = s] \cdot \Pr[\omega_0 = \omega, S= s]
=& \textstyle \sum_s \Pr[S' = s'\mid S=s]
    \cdot \Pr[\omega_0 = \omega\mid S=s] \cdot \Pr[S=s] \\
=& \textstyle  \sum_s \Pr[S'=s,\omega_0 =\omega\mid S=s] \cdot \Pr[S=s] \\
= &\textstyle  \sum_s \Pr[S=s,S'=s',\omega_0 =\omega] \\
= &\Pr[\omega_0 =\omega, S'=s].
\end{align*}

Therefore $\pi'$ being BIC implies that $\pi$ is also BIC. Indeed, for any $a,a' \in \A$ and $\theta \in \varTheta$,
\begin{align*}
&\textstyle \sum_{\omega,s}\; \Pr[\omega_0 = \omega, S = s] \cdot (u(\theta,a', \omega) - u(\theta,a,\omega)) \cdot \Pr[\pi(s) = a] \\
=& \textstyle \sum_{\omega,s'}\;\Pr[\omega_0 = \omega, S' = s'] \cdot (u(\theta,a', \omega) - u(\theta,a,\omega)) \cdot \Pr[\pi(s') = a]
\geq 0.
\end{align*}

%It's easy to check $\pi$ is BIC.

Finally, for any $s', s ,a$ such that $Pr[S' = s',S = s] >0 $ and $\Pr[\pi'(s') = a] >0$, we have $\Pr[\pi(s) = a] > 0$. This implies $\EX_{s'}[\S'] \subseteq \EX_s[\S]$.
\end{proof}

\xhdr{Max-Support Policy.}
We can solve the following LP to check whether a particular action $a_0 \in\A$ is signal-explorable given a particular realized signal $s_0\in\X$. In this LP, we represent a policy $\pi$ as a set of numbers
    $x_{a,s} = \Pr[\pi(s)=a]$,
for each action $a\in \A$ and each feasible signal $s\in \X$.


\begin{figure}[H]
\begin{mdframed}
\vspace{-5mm}
\begin{alignat*}{2}
\textbf{maximize }   & x_{a_0,s_0}\  \\
\textbf{subject to: }
    & \textstyle \sum_{\omega \in \varOmega, s \in \X} \;
    \Pr[\omega] \cdot \Pr[s \mid  \omega] \cdot
        \left(u(\theta, a, \omega) - u(\theta, a', \omega)\right) \cdot x_{a,s} \geq 0   &\ & \forall a,a' \in \A \\
    & \textstyle \sum_{a\in \A}\; x_{a,s} = 1,  \ &\ & \forall s \in \X \\
    & x_{a,s} \geq 0,  \ &\ & \forall s \in \X, a\in \A
\end{alignat*}
\end{mdframed}
%\caption{LP}
\label{fig:public_lp}
\end{figure}

Since the constraints in this LP characterize any BIC recommendation policy, it follows that action $a_0$ is signal-explorable given realized signal $s_0$ if and only if the LP has a positive solution. If such solution exists, define recommendation policy $\pi = \pi^{a_0,s_0}$ by setting
    $\Pr[\pi(s) = a] = x_{a,s}$ for all $a\in \A, s\in \X$.
Then this is a BIC recommendation policy such that
    $\Pr[\pi(s_0) = a_0] > 0$.

\begin{definition}
Given a signal structure $\S$, a BIC recommendation policy $\pi$ is called  \emph{max-support} if $\forall s \in \X$  and signal-explorable action $a\in \A$ given $s$, $\Pr[\pi(s) = a] > 0$.
\end{definition}


It is easy to see that we obtain max-support recommendation policy by averaging the $\pi^{a,s}$ policies defined above. Specifically, the following policy is BIC and max-support:
\begin{align}\label{eq:pimax}
\pi^{\max} = \frac{1}{|\X|} \sum_{s \in \X} \frac{1}{|\EX_s[\S]|} \sum_{a \in \EX_s[\S]} \pi^{a,s}.
\end{align}

%
%We can get a max-support policy $\pi^{\max}$ by averaging over BIC recommendation policies for signal explorable actions.
%\begin{claim}
%\label{clm:pimax}
%The following policy is BIC and max-support:
%\[
%\pi^{\max} = \frac{1}{|\X|} \sum_{s \in \X} \frac{1}{|\EX_s[\S]|} \sum_{a \in \EX_s[S]} \pi^{a,s}.
%\]
%\end{claim}

\xhdr{Maximal Exploration.}
%\label{sec:public_maxe}
%This section is very similar to the EC16's paper, we include this section to make our paper self-contained.
We design a subroutine \term{MaxExplore} which outputs a sequence of actions with two properties: it includes every signal-explorable action at least once, and the marginal distribution at each location is $\pi^{\max}$. The length of this sequence, denoted $L_{\theta}$, should satisfy
\begin{align}\label{eq:public-L}
L_{\theta} \geq \max_{(a,s)\in \A\times \X \text{ with }\quad \Pr[\pi^{\max}(s)=a] \neq 0} \frac{1}{\Pr[\pi^{\max}(s)=a]}.
\end{align}

The idea is to put $C_a = L_{\theta} \cdot \Pr[\pi^{\max}(S) = a]$ copies of each action $a$ into a sequence of length $L_{\theta}$ and randomly permute the sequence. However, $C_a$ might not be an integer, and in particular may be smaller than 1.The latter issue is resolved by making $L_{\theta}$ sufficiently large. For the former issue, we first put $\lfloor C_a \rfloor$ copies of each action $a$ into the sequence, and then sample the remaining
    $L_\theta - \sum_a \lfloor C_a \rfloor$
actions according to distribution
    $\pRes(a) = \frac{C_a - \lfloor C_a \rfloor}{L_\theta - \sum_a \lfloor C_a \rfloor}$.
For details, see Algorithm \ref{alg:public_explore}.
 \begin{algorithm}[H]
    \caption{Subroutine MaxExplore}
    	\label{alg:public_explore}
    \begin{algorithmic}[1]
	\STATE \textbf{Input:} type $\theta$, realized signal $S$ and signal structure $\S$.
	\STATE \textbf{Output:} a list of actions $\alpha$
	\STATE Compute $\pi^{\max}$ as per \eqref{eq:pimax}
	%\IF {$l \leq |\A| \cdot |\varTheta|$}
		\STATE Initialize $Res = L_{\theta}$.
		\FOR {each action $ a \in \A$}
							\STATE $C_a \leftarrow  L_{\theta} \cdot \Pr[\pi^{\max}(S) = a]$
                     		\STATE Add $\lfloor C_a \rfloor$ copies of action $a$ into list $\alpha$.
			\STATE $Res \leftarrow Res -\lfloor C_a \rfloor $.
			\STATE $\pRes(a)\leftarrow  C_a -  \lfloor C_a\rfloor$
		\ENDFOR
		\STATE $\pRes(a) \leftarrow \pRes(a) / Res$, $\forall a \in \A$.
		\STATE Sample $Res$ many actions from distribution according to $\pRes$ independently and add these actions into $\alpha$.
		\STATE Randomly permute the actions in $\alpha$.
	%\ELSE
		%\STATE Add $L_{\theta}$ copies of the best explored action of type $\theta$ according to signal $S$ into action list $\alpha$.
	%\ENDIF
	\RETURN $\alpha$.	
     \end{algorithmic}
\end{algorithm}

Such conversion is exactly the same as \cite{ICexplorationGames-ec16-working}, we have the following claim.

\begin{claim}
\label{clm:maxexplore}
Given type $\theta$ and realized signal $S$, MaxExplore outputs a sequence of $L_{\theta}$ actions.
Each action in the sequence marginally distributed as $\pi^{\max}$.
For any action $a$ such that $\Pr[\pi^{\max} =a] >0$, $a$ shows up in the sequence at least once with probability exactly 1.
MaxExplore runs in time polynomial in $L_{\theta}$, $|\A|$, $|\varOmega|$ and $|\X|$ (size of the support of the signal).
\end{claim}

\subsection{Main Recommendation Policy}
\label{sec:public_main}

Algorithm \ref{alg:public_main} is the main procedure of our recommendation policy. It consists of two parts: \emph{exploration}, which explores all the eventually-explorable actions, and \emph{exploitation}, which simply recommends the best explored action for a given type. The exploration part proceeds in phases. In each phase $\ell$, each type $\theta$ gets a sequence of $L_{\theta}$ actions from MaxExplore using the data collected before this phase starts. The phase ends when every agent type $\theta$ has finished $L_{\theta}$ rounds.  After $|\A| \cdot \varTheta|$ phases, our recommendation policy enters the exploitation part. See Algorithm \ref{alg:public_main} for  details.

There is a separate thread for each type $\theta$, denoted $\thread(\theta)$,  which is called whenever an agent of this type shows up. For each agent type $\theta$, we pick parameter $L_{\theta}$ to be large enough so that the condition
\eqref{eq:public-L} is satisfied for all phases $\ell$ and all possible signals $S=S_\ell$. (Note that $L_{\theta}$ is finite because there are only finitely many such signals.)

 \begin{algorithm}[H]
    \caption{Main procedure for public types }
    	\label{alg:public_main}
    \begin{algorithmic}[1]
    \STATE Initialization: signal $S_1 = \S_1 = \perp$,
             phase count $\ell = 1$, index $i_{\theta} = 0$ for each type $\theta \in \varTheta$.
	\FOR {rounds $t=1$ to $T$}
		\IF {$\ell \leq |\A|\cdot |\varTheta|$}
		 \STATE \COMMENT{Exploration}
		Call thread $\thread(\theta_t)$.
			\IF {every type $\theta$ has finished $L_{\theta}$ rounds in the current phase ($i_{\theta} \geq L_{\theta}$)}
				\STATE Start a new phase: $\ell \leftarrow \ell + 1$.
				\STATE Let $S_\ell$ be the signal for phase $\ell$: 
                 the set of all observed type-action-reward triples.
        \STATE Let $\S_\ell$ be the signal structure for $S_\ell$ given the realized type sequence $(\theta_1,...,\theta_t)$.
			\ENDIF
		\ELSE
			\STATE \COMMENT{Exploitation}
			Recommend the best explored action for agent type $\theta_t$.
		\ENDIF
	\ENDFOR
     \end{algorithmic}
\end{algorithm}

The thread for type $\theta$ is given in Algorithm \ref{alg:public_sub}. In a given phase, it recommends the $L_{\theta}$ actions computed by MaxExplore, then switches to the best explored action. The thread only uses the information collected before the current phase starts: the signal $S_\ell$ and signal structure $\S_\ell$.

 \begin{algorithm}[H]
    \caption{Thread for agent type $\theta$: $\thread(\theta)$ }
    	\label{alg:public_sub}
    \begin{algorithmic}[1]
	%\FOR {each call from Algorithm \ref{alg:public_main}}
		\IF {this is the first call of $\thread(\theta)$ of the current phase}
			\STATE Compute a list of $L_{\theta}$ actions $\alpha_{\theta} \leftarrow $ MaxExplore($\theta, S_\ell, \S_\ell$).
			\STATE Initialize the index of type $\theta$: $i_{\theta} \leftarrow 0$.
		\ENDIF
		\STATE $i_{\theta} \leftarrow i_{\theta} + 1$.
		\IF {$i_{\theta} \leq L_{\theta}$}
			\STATE Recommend action $\alpha_{\theta} [i_{\theta}]$.
		\ELSE
			\STATE Recommend the best explored action of type $\theta$.
		\ENDIF
	%\ENDFOR
     \end{algorithmic}
\end{algorithm}

The BIC property follows easily from Claim \ref{clm:maxexplore}.

\OMIT{The performance analysis proceeds as follows. First, we upper-bound the expected number of rounds of a phase (Lemma \ref{lem:epoch}). Then we show, in Lemma \ref{lem:exp_public}, that Algorithm \ref{alg:public_main} explores all  eventually-explorable type-action pairs in $|\A| \cdot |\varTheta|$ phases. We use these two lemmas to prove the main theorem.}

\begin{lemma}
\label{lem:epoch}
The expected number of rounds in each phase $\ell$ at most
$ \sum_{\theta\in\varTheta} \frac{L_{\theta}}{\Pr[\theta]}$.
\end{lemma}

\begin{proof}
Phase ends as soon as each type has shown up at least $L_{\theta}$ times. The expected number of rounds by which this happens is at most
$ \sum_{\theta\in\varTheta} \frac{L_{\theta}}{\Pr[\theta]}$.
\end{proof}

\OMIT{
Notice that in Algorithm \ref{alg:public_main} the partition of phases depends only on realized types $\theta_1,...,\theta_T$.
\begin{claim}
Given the sequence $\theta_1,...,\theta_T$, the partition of phases in Algorithm \ref{alg:public_main} is fixed.
\end{claim}
} %%%%%

The key argument is that Algorithm \ref{alg:public_main} explores all  eventually-explorable type-action pairs.

\begin{lemma}
\label{lem:exp_public}
Fix phase $\ell>0$ and the sequence of agent types $\theta_1,...,\theta_T$. Assume Algorithm \ref{alg:public_main} has been running for at least $\min(l, |\A|\cdot |\varTheta|)$ phases.
For a given state $\omega$, if type-action pair $(\theta,a)$ can be explored by some BIC recommendation policy $\pi$ at round $\ell$ with positive probability, then such action is explored by Algorithm \ref{alg:public_main} by the end of phase $\min(l, |\A|\cdot |\varTheta|)$ with probability $1$.
\end{lemma}

\begin{proof}
We prove this by induction on $\ell$ for $\ell \leq |\A|\cdot |\varTheta|$. Base case $\ell=1$ is trivial by Claim \ref{clm:maxexplore}. Assuming the lemma is correct for $\ell-1$, let's prove it's correct for $\ell$.

Let $S= S_l$ be the signal of Algorithm \ref{alg:public_main} by the end of phase $\ell-1$.  Let $S'$ be the history of $\pi$ in the first $\ell-1$ rounds. More precisely,
    $S' = (R, H_1,...,H_{l-1})$,
where $R$ is the internal randomness of policy $\pi$, and
    $H_t = (\Theta_t, A_t, u(\Theta_t, A_t, \omega_0))$
is the type-action-reward triple in round $t$ of policy $\pi$.

The proof plan is as follows. We first show that $I(S';\omega_0|S) =0 $. Informally, this means the information collected in the first $l-1$ phases of Algorithm \ref{alg:public_main} contains all the information $S'$ has about the state $w_0$. After that, we will use the information monotonicity lemma to show that phase $l$ of Algorithm \ref{alg:public_main} explores all the action-type pairs $\pi$ might explore in round $l$.

First of all, we have
\begin{align*}
I(S'; \omega_0\mid  S)
    &= I(R,H_1,...,H_{l-1}; \omega_0\mid  S)
    = I(R; \omega_0\mid  S) + I(H_1,...,H_{l-1}; \omega_0\mid S, R) \\
    &= I(H_1,...,H_{l-1}; \omega_0\mid S, R).
\end{align*}

By the chain rule of mutual information, we have
\[
 I(H_1,...,H_{l-1}; \omega_0\mid S, R) = I(H_1;\omega_0\mid S,R) + I(H_2;\omega_0\mid S, R ,H_1) + \cdots + I(H_{l-1}; \omega_0\mid S,R,H_1,...,H_{l-2}).
\]

For all $t \in [l-1]$, we have
\begin{align*}
&I(H_t; \omega_0\mid S,R,H_1,...,H_{t-1}) \\
=& I(\Theta_t, A_t, u(\Theta_t, A_t, \omega_0); \omega_0\mid S,R,H_1,...,H_{t-1}) \\
=& I(\Theta_t ; \omega_0\mid S,R,H_1,...,H_{t-1}) +  I(A_t, u(\Theta_t, A_t, \omega_0); \omega_0\mid S,R,H_1,...,H_{t-1},\Theta_t) \\
=& I(A_t, u(\Theta_t, A_t, \omega_0); \omega_0\mid S,R,H_1,...,H_{t-1},\Theta_t).
\end{align*}
Notice that the suggested action $A_t$ is a deterministic function of randomness of the recommendation policy $R$,  history of previous rounds $H_1,...,H_{t-1}$ and type in the current round $\Theta_t$. Also notice that, by induction hypothesis, $u(\Theta_t, A_t, \omega_0)$ is a deterministic function of $S,R,H_1,...,H_{t-1},\Theta_t, A_t$. Therefore we have
\[
I(H_t; \omega_0\mid S,R,H_1,...,H_{t-1}) = 0, \qquad \forall t \in [l-1].
\]
Then we get
$ I(S'; \omega_0 \mid  S) = 0.$

By Lemma \ref{lem:infomono}, we know that $\EX[\S'] \subseteq \EX[\S]$. For state $\omega$, there exists a signal $s'$ such that $\Pr[S'=s'\mid \omega_0 =\omega] >0 $ and $a \in \EX_{s'} [\S']$. Now let $s$ be the realized value of $S$ given $\omega_0 = \omega$, we know that $\Pr[S'=s'\mid S=s] >0$, so $a \in \EX_s[\S]$. By Claim \ref{clm:maxexplore}, we know that at least one agent of type $\theta$ in phase $\ell$ of Algorithm \ref{alg:public_main} will choose action $a$.

Now we consider the case when $\ell > |\A| \cdot |\varTheta|$. Define Algorithm $E$ to be the variant of Algorithm \ref{alg:public_main} such that it only does exploration (removing the if-condition and exploitation in Algorithm \ref{alg:public_main}). For $\ell > |\A| \cdot |\varTheta|$, the above induction proof still work for Algorithm $E$, i.e. for a given state $\omega$, if an action $a$ of type $\theta$ can be explored by a BIC recommendation policy $\pi$ at round $\ell$, then such action is guaranteed to be explored by Algorithm $E$ by the end of phase $\ell$. Now we are going to argue that Algorithm $E$ won't explore any new action-type pairs after phase $|\A| \cdot |\varTheta|$. Call a phase exploring if in that phase Algorithm $E$ explores at least one new action-type pair. As there are  $ |\A| \cdot |\varTheta|$ type-action pairs, Algorithm $E$ can have at most $ |\A| \cdot |\varTheta|$ exploring phases. On the other hand, once Algorithm $E$ has a phase that is not exploring, because the signal stays the same after that phase, all phases afterwards are not exploring. Therefore Algorithm $E$ does not have any exploring phases after phase $|\A| \cdot |\varTheta|$. For $\ell > |\A| \cdot |\varTheta|$, the first $|   \A| \cdot |\varTheta|$ phases of Algorithm \ref{alg:public_main} explores the same set of type-action pairs as the first $\ell$ phases of Algorithm $E$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:public}]
Algorithm \ref{alg:public_main} is BIC  by Claim \ref{clm:maxexplore}. By Lemma \ref{lem:exp_public}, for each state $\omega$, Algorithm \ref{alg:public_main} explores all the eventually explorable actions (i.e. $\AExp_{\omega,\theta}$) for each type $\theta$ by the end of $|\A|\cdot |\varTheta|$ phases. After that, for each type $\theta$, Algorithm \ref{alg:public_main} will always recommend action $ \arg\max_{a \in \AExp_{\omega,\theta}} u(\theta, a, \omega)$. Therefore Algorithm \ref{alg:public_main} gets reward $\OPT$ except rounds in first $|\A|\cdot |\varTheta|$ phases. By Lemma \ref{lem:epoch}, the expected number of rounds of the first  $|\A|\cdot |\varTheta|$ phases does not depend on the time horizon $T$. Therefore, Algorithm \ref{alg:public_main} has expected total reward at least  $\left(T - C \right) \cdot \OPT$.
\end{proof} 