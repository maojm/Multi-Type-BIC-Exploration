%!TEX root = main.tex

\section{Bayesian Exploration with Public Types}
\label{sec:public}
In this section, we show a BIC recommendation policy for public types (as in Theorem \ref{thm:public}). We formally state Theorem \ref{thm:public} in Section \ref{sec:public_bench} and we prove it in Section \ref{sec:public_single} and \ref{sec:public_main}.

\subsection{Explorability and Benchmark}
\label{sec:public_bench}
In this subsection, we define the benchmark and state our main theorem (Theorem \ref{thm:public}).

\begin{definition}
\label{def:public_exp}
An action $a$ of type $\theta$ is eventually-explorable, for a given state $\omega$, if there exists a BIC recommendation policy $\pi$ and some round $t$ such that $\Pr[\pi^t(\theta)= a]> 0$. The set of all such actions for state $\omega$ and type $\theta$ is denoted as $\A_{\omega,\theta}^{exp}$.
\end{definition}

\begin{definition}[Benchmark]
Define benchmark as
\[
\OPT = \sum_{\theta \in \varTheta, \omega\in \varOmega} \Pr[\omega] \cdot \Pr[\theta] \cdot \max_{a \in \A_{\omega,\theta}^{exp}} u(\theta, a, \omega).
\]
\end{definition}

Notice that for a given state $\omega$ and type $\theta$, any BIC recommendation policy can only recommend an action in $\A_{\omega,\theta}^{exp}$. We can simply get the following claim which says that no BIC recommendation policy can get expected per round reward better than the benchmark:
\begin{claim}
Any BIC recommendation policy of $T$ rounds has expected total reward at most $T \cdot\OPT$.
\end{claim}

On the other hand, we construct a BIC recommendation policy that nearly achieves the benchmark. It's proved in the following subsections.
\begin{theorem}
\label{thm:public}
We have a BIC recommendation policy of $T$ rounds with expected total reward at least $\left(T - C \right) \cdot \OPT$ for some constant $C$ which does not depend on $T$.
\end{theorem}

\subsection{Single-round Exploration}
\label{sec:public_single}
%This section is very similar to the EC16's paper, we include this section to make our paper self-contained.

In this subsection, we consider a single round of the Bayesian exploration. As we only consider one round, we fix the agent's type in this round to be $\theta$.

In a single round, the principal receives a random variable $S$ (we call it a signal) which is a function of the history of previous rounds and randomness of the recommendation policy. This function is determined before all rounds start and is known to the principal and all agents. In this single round, the principal computes a (randomized) mapping from the signal to a suggested action. We specify the signal structure $\S$ of signal $S$ by the signal support $\X$ and a joint distribution on $(\X, \varOmega)$.


\begin{definition}
Consider a single-round of Bayesian exploration when the principal receives signal $S$ with signal structure $\S$. An action $a \in \A$ is called signal-explorable, for a given type $\theta$ and a given realized signal $s$, if there exists a single-round BIC recommendation policy $\pi$ such that $\Pr[\pi(s) = a] > 0$. The set of all such actions is denoted as $\EX_s[\S]$. The signal-explorable set is defined as $\EX[\S] = \EX_S[\S]$.
\end{definition}

Note that $\EX_s[\S]$ is a fixed subset and $\EX[\S]$ is a random variable whose realization is determined by the realization of signal $S$.

\xhdr{Information Monotonicity.}
In the following definition, we define a way to compare two signals. Intuitively, the condition $I(S';\Omega|S)= 0$  means if one is given random variable $S$, one can learn no information from $S'$ about $\Omega$. In this case, we show in Lemma \ref{lem:infomono} that the signal-explorable set of $S'$ is contained the signal-explorable set of $S$.
\begin{definition}
Let $S$ and $S'$ be two random variables and $\S$ and $\S'$ be their signal structures. We say random variable $S$ is at least as informative as random variable $S'$ about state $\Omega$ if $I(S' ; \Omega|S) = 0$. Notice $I(S';\Omega|S)$ does not only depend on the signal structures $\S$ and $\S'$. It can also depend on the joint distribution between $S$ and $S'$.
\end{definition}

\begin{lemma}
\label{lem:infomono}
Let $S$ and $S'$ be two random variables and $\S$ and $\S'$ be their signal structures. If $S$ is at least as informative as $S'$ about state $\Omega$ (i.e. $I(S' ; \Omega|S) = 0$), then $\EX_{s'}[\S'] \subseteq \EX_s[\S]$ for all $s' ,s$ such that $\Pr[S= s, S'= s'] > 0$.
\end{lemma}

\begin{proof}
Consider any BIC recommendation policy $\pi'$ for signal structure $\S'$. We construct $\pi$ for signature structure $\S$ by setting $\Pr[\pi(s) = a] = \sum_{s'} \Pr[\pi'(s') = a] \cdot Pr[S' = s'|S = s]$. Notice that $I(S' ; \Omega|S) = 0$ implies $S'$ and $\Omega$ are independent given $S$, i.e $\Pr[S' = s'|S=s] \cdot \Pr[\Omega = \omega|S=s] = \Pr[S'=s', \Omega = \omega|S=s]$ for all $s,s',\omega$. Therefore, for all $s'$ and $\omega$,  
\begin{align*}
 &\sum_s \Pr[S' = s'|S = s] \cdot \Pr[\Omega = \omega, S= s] \\
=& \sum_s   \Pr[S' = s'|S=s] \cdot \Pr[\Omega = \omega|S=s] \cdot \Pr[S=s] \\
= &\sum_s \Pr[S'=s,\Omega =\omega|S=s] \cdot \Pr[S=s] \\
= &\sum_s \Pr[S=s,S'=s',\Omega =\omega] \\
= &\Pr[\Omega =\omega, S'=s].\\
\end{align*}

 Therefore $\pi'$ being BIC implies that $\pi$ is also BIC. More specifically, for any $a,a' \in \A$ and $\theta \in \varTheta$,  
\begin{align*}
&\sum_{\omega,s} \Pr[\Omega = \omega, S = s] \cdot (u(\theta,a', \omega) - u(\theta,a,\omega)) \cdot \Pr[\pi(s) = a] \\
=&\sum_{\omega,s'}\Pr[\Omega = \omega, S' = s'] \cdot (u(\theta,a', \omega) - u(\theta,a,\omega)) \cdot \Pr[\pi(s') = a] \\
\geq & 0.
\end{align*} 

%It's easy to check $\pi$ is BIC. 

Finally, for any $s', s ,a$ such that $Pr[S' = s',S = s] >0 $ and $\Pr[\pi'(s') = a] >0$, we have $\Pr[\pi(s) = a] > 0$. This implies $\EX_{s'}[\S'] \subseteq \EX_s[\S]$.
\end{proof}

\xhdr{Max-Support Policy.}
We can solve the following LP to check whether a particular action $a_0 \in\A$ is signal-explorable given a particular realized signal $s_0\in\X$. In this LP, we represent a policy $\pi$ as a set of numbers
    $x_{a,s} = \Pr[\pi(s)=a]$,
for each action $a\in \A$ and each feasible signal $s\in \X$. 


\begin{figure}[H]
\begin{mdframed}
\vspace{-5mm}
\begin{alignat*}{2}
\textbf{maximize }   & x_{a_0,s_0}\  \\
\textbf{subject to: }
    & \textstyle \sum_{\omega \in \varOmega, s \in \X} \;
    \Pr[\omega] \cdot \Pr[s | \omega] \cdot 
        \left(u(\theta, a, \omega) - u(\theta, a', \omega)\right) \cdot x_{a,s} \geq 0   &\ & \forall a,a' \in \A \\
    & \textstyle \sum_{a\in \A}\; x_{a,s} = 1,  \ &\ & \forall s \in \X \\
    & x_{a,s} \geq 0,  \ &\ & \forall s \in \X, a\in \A
\end{alignat*}
\end{mdframed}
%\caption{LP}
\label{fig:public_lp}
\end{figure}

Since the constraints in this LP characterize any BIC recommendation policy, it follows that action $a_0$ is signal-explorable given realized signal $s_0$ if and only if the LP has a positive solution. If such solution exists, define recommendation policy $\pi = \pi^{a_0,s_0}$ by setting 
    $\Pr[\pi(s) = a] = x_{a,s}$ for all $a\in \A, s\in \X$. 
Then this is a BIC recommendation policy such that 
    $\Pr[\pi(s_0) = a_0] > 0$.

\begin{definition}
Given a signal structure $\S$, a BIC recommendation policy $\pi$ is called  \emph{max-support} if $\forall s \in \X$  and signal-explorable action $a\in \A$ given $s$, $\Pr[\pi(s) = a] > 0$.
\end{definition}


It is easy to see that we obtain max-support recommendation policy by averaging the $\pi^{a,s}$ policies defined above. Specifically, the following policy is BIC and max-support:
\begin{align}\label{eq:pimax}
\pi^{max} = \frac{1}{|\X|} \sum_{s \in \X} \frac{1}{|\EX_s[\S]|} \sum_{a \in \EX_s[\S]} \pi^{a,s}.
\end{align}

%
%We can get a max-support policy $\pi^{max}$ by averaging over BIC recommendation policies for signal explorable actions.
%\begin{claim}
%\label{clm:pimax}
%The following policy is BIC and max-support:
%\[
%\pi^{max} = \frac{1}{|\X|} \sum_{s \in \X} \frac{1}{|\EX_s[\S]|} \sum_{a \in \EX_s[S]} \pi^{a,s}.
%\]
%\end{claim}

\xhdr{Maximal Exploration.}
%\label{sec:public_maxe}
%This section is very similar to the EC16's paper, we include this section to make our paper self-contained.
Let us design a subroutine, called  MaxExplore, which outputs a sequence of $L_{\theta}$ actions for type $\theta$. We are going to assume $L_{\theta} \geq \max_{a,s: \Pr[\pi^{max}(s)=a] \neq 0} \frac{1}{\Pr[\pi^{max}(s)=a]}$.

The goal of this subroutine MaxExplore is to make sure that for any signal-explorable action $a$ (i.e. $\Pr[\pi^{max}(s) = a] > 0$), $a$ shows up at least once in the sequence with probability exactly 1. On the other hand, we want that the action of each specific location in the sequence has marginal distribution same as $\pi^{max}$.

To achieve this goal, the first idea would be to put $C_a = L_{\theta} \cdot \Pr[\pi^{max}(S) = a]$ copies of action $a$ into a sequence of length $L_{\theta}$ and randomly permute the sequence. However, the problems are that $C_a$ might not be an integer and $C_a$ can be smaller than 1. We solve the later problem by picking $L_{\theta}$ large enough such that $C_a \geq 1$ for all $a$ satisfying $\Pr[\pi^{max}(S) =a] > 0$. We solve the former problem by first putting $\lfloor C_a \rfloor$ copies of action $a$ into the sequence and then sampling the rest ($L_\theta - \sum_a \lfloor C_a \rfloor$ actions) according to $p^{Res}$ such that $p^{Res}(a) = \frac{C_a - \lfloor C_a \rfloor}{L_\theta - \sum_a \lfloor C_a \rfloor}$. For details, see the following pseudo-code (Algorithm \ref{alg:public_explore}).
 \begin{algorithm}[H]
    \caption{Subroutine MaxExplore}
    	\label{alg:public_explore}
    \begin{algorithmic}[1]
	\STATE \textbf{Input:} type $\theta$, realized signal $S$ and signal structure $\S$.
	\STATE \textbf{Output:} a list of actions $\alpha$
	\STATE Compute $\pi^{max}$ as per \eqref{eq:pimax}
	%\IF {$l \leq |\A| \cdot |\varTheta|$}
		\STATE Initialize $Res = L_{\theta}$.
		\FOR {each action $ a \in \A$}
							\STATE $C_a \leftarrow  L_{\theta} \cdot \Pr[\pi^{max}(S) = a]$
                     		\STATE Add $\lfloor C_a \rfloor$ copies of action $a$ into list $\alpha$.
			\STATE $Res \leftarrow Res -\lfloor C_a \rfloor $.
			\STATE $p^{Res}(a)\leftarrow  C_a -  \lfloor C_a\rfloor$
		\ENDFOR
		\STATE $p^{Res}(a) \leftarrow p^{Res}(a) / Res$, $\forall a \in \A$.
		\STATE Sample $Res$ many actions from distribution according to $p^{Res}$ independently and add these actions into $\alpha$.
		\STATE Randomly permute the actions in $\alpha$.
	%\ELSE
		%\STATE Add $L_{\theta}$ copies of the best explored action of type $\theta$ according to signal $S$ into action list $\alpha$.
	%\ENDIF
	\RETURN $\alpha$.	
     \end{algorithmic}
\end{algorithm}

Such conversion is exactly the same as \cite{ICexplorationGames-ec16-working}, we have the following claim.

\begin{claim}
\label{clm:maxexplore}
Given type $\theta$ and realized signal $S$, MaxExplore outputs a sequence of $L_{\theta}$ actions. 
Each action in the sequence marginally distributed as $\pi^{max}$. 
For any action $a$ such that $\Pr[\pi^{max} =a] >0$, $a$ shows up in the sequence at least once with probability exactly 1.
MaxExplore runs in time polynomial in $L_{\theta}$, $|\A|$, $|\varOmega|$ and $|\X|$ (size of the support of the signal).
\end{claim}

\subsection{Main Recommendation Policy}
\label{sec:public_main}
In this subsection, we show our main recommendation policy. Algorithm \ref{alg:public_main} is the main procedure of our recommendation policy. It contains two parts: exploration and exploitation. The exploration explores all the eventually-explorable actions and we will give more explanation below. The exploitation part simply advises the agents to choose the best actions among eventually-explorable actions. 

The exploration part proceeds in phases. In a phase, each type $\theta$ gets a sequence of $L_{\theta}$ actions from MaxExplore using data collected before this phase starts. These $L_{\theta}$ actions are suggested to the first $L_{\theta}$ agents of type $\theta$ in the phase. The phase ends when every type $\theta$ has finished $L_{\theta}$ rounds in the current phase. If in a phase some type $\theta$ has already finished $L_{\theta}$ rounds but the phase hasn't ended, our recommended policy simply suggests the best explored action of type $\theta$ to other agents of type $\theta$. And finally after $|\A| \cdot \varTheta|$ phases, our recommendation policy enters the exploitation part.

See the pseudocode (Algorithm \ref{alg:public_main}) for more details. For implementation, in the exploration part, our recommendation policy activates a separate thread (Sub-$\theta$) whenever type $\theta$ shows up in a round and deactivates the thread after that round. We pick $L_{\theta}$ to be at least $\max_{a,s: \Pr[\pi(s)=a] \neq 0} \frac{1}{\Pr[\pi(s)=a]}$ for all $\pi$ that might be chosen as $\pi^{max}$ in Algorithm \ref{alg:public_main} such that $L_{\theta}$ satisfies the guarantee required in Section \ref{sec:public_single}.

 \begin{algorithm}[H]
    \caption{Main procedure for public types }
    	\label{alg:public_main}
    \begin{algorithmic}[1]
    	\STATE Initial signal $S_1 = \S_1 = \perp$.
	\STATE Initial phase count $l = 1$.
	\STATE Initial index of each type $i_{\theta} = 0$ for all $\theta \in \varTheta$.
	\FOR {$t=1$ to $T$}
		\IF {$l \leq |\A|\cdot \varTheta|$}
			\STATE \textbf{Exploration:}
			\STATE Let $\theta_t$ to be the agent type of current round $t$. Run subroutine Sub-$\theta_t$.
			\IF {every type $\theta$ has finished $L_{\theta}$ rounds in the current phase, i.e. $i_{\theta} \geq L_{\theta}$ for all $\theta$}
				\STATE Start a new phase:
				\STATE $l \leftarrow l + 1$.
				\STATE Let $S_l$ be the set of type-action-reward triples in $t$ rounds and $\S_l$ be the signal structure when fixing $\theta_1,...,\theta_t$.
			\ENDIF
		\ELSE
			\STATE \textbf{Exploitation:} 
			\STATE Suggest the best explored action of type $\theta$ to the agent.
		\ENDIF
	\ENDFOR
     \end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:public_sub} describes the thread for type $\theta$. It is activated whenever the agent in the current round has type $\theta$. In a phase, it first suggests $L_{\theta}$ actions computed by MaxExplore. After that it suggests the best explored action to agents. The thread only uses the information collected before the current phase starts.

 \begin{algorithm}[H]
    \caption{Subroutine for type $\theta$: Sub-$\theta$ }
    	\label{alg:public_sub}
    \begin{algorithmic}[1]
	\FOR {each call from Algorithm \ref{alg:public_main}}
		\IF {this is the first call of the current phase}
			\STATE Use the current $S_l$ and $\S_l$ to compute a list of $L_{\theta}$ actions $\alpha_{\theta} \leftarrow $ MaxExplore($\theta, S_l, \S_l$).
			\STATE Initialize the index of type $\theta$: $i_{\theta} \leftarrow 0$.
		\ENDIF
		\STATE $i_{\theta} \leftarrow i_{\theta} + 1$.
		\IF {$i_{\theta} \leq L_{\theta}$}
			\STATE Suggest action $\alpha_{\theta} [i_{\theta}]$ to the agent.
		\ELSE
			\STATE Suggest the best explored action of type $\theta$ to the agent.
		\ENDIF
	\ENDFOR
     \end{algorithmic}
\end{algorithm}


The proof plan is to first give an upper bound on the expected number of rounds of a phase in Lemma \ref{lem:epoch} and then show in Lemma \ref{lem:exp_public} that Algorithm \ref{alg:public_main} explores all the explorable type-action pairs in $|\A| \cdot |\varTheta|$ phases. We use these two lemmas to prove our main theorem (Theorem \ref{thm:public}) in Corollary \ref{cor:public}.

First of all, it's easy to check by Claim \ref{clm:maxexplore} that for each agent, it is Bayesian incentive compatible to follow the recommended action if previous agents all follow the recommended actions. We have the following claim.
\begin{claim}
\label{clm:public_BIC}
Our recommendation policy in this section is BIC.
\end{claim}

\begin{lemma}
\label{lem:epoch}
For any $l>0$, the expected number of rounds of the first $l$ phases is at most $l \cdot \sum_{\theta\in\varTheta} \frac{L_{\theta}}{\Pr[\theta]}$.
\end{lemma}

\begin{proof}
The expected number of rounds of a phase is at most the expected number of rounds in which each type $\theta$ has shown up at least $L_{\theta}$ times. The latter expectation can be easily bounded by $\sum_{\theta\in\varTheta} \frac{L_{\theta}}{\Pr[\theta]}$. Therefore, the expected number of rounds of the first $l$ phases is at most $l \cdot \sum_{\theta\in\varTheta} \frac{L_{\theta}}{\Pr[\theta]}$.
\end{proof}

Notice that in Algorithm \ref{alg:public_main} the partition of phases depends only on realized types $\theta_1,...,\theta_T$. We then have the following claim.
\begin{claim}
Given $\theta_1,...,\theta_T$ as the realized types in $T$ rounds, the partition of phases in Algorithm \ref{alg:public_main} is fixed.
\end{claim}

\begin{lemma}
\label{lem:exp_public}
For any $l>0$ and a fixed sequence of types $\theta_1,...,\theta_T$ as the types encountered by Algorithm \ref{alg:public_main}, assume Algorithm \ref{alg:public_main} has at least $\min(l, |\A|\cdot |\varTheta|)$ phases.
For a given state $\omega$, if an action $a$ of type $\theta$ can be explored by a BIC recommendation policy $\pi$ at round $l$ (i.e. $ \Pr[\pi^l(\theta)= a]> 0$), then such action is guaranteed to be explored by Algorithm \ref{alg:public_main} by the end of phase $\min(l, |\A|\cdot |\varTheta|)$.
\end{lemma}

\begin{proof}
We prove this by induction on $l$ for $l \leq |\A|\cdot |\varTheta|$. Base case $l=1$ is trivial by Claim \ref{clm:maxexplore}. Assuming the lemma is correct for $l-1$, let's prove it's correct for $l$.

Let $S$ be the history of Algorithm \ref{alg:public_main} by the end of phase $l-1$. By the definition of Algorithm 2, $S = S_{l-1} | \theta_1,...,\theta_T$.  Let $S'$ be the history of $\pi$ in the first $l-1$ rounds. More precisely, $S' = R, H_1,...,H_{l-1}$. Here $R$ is the internal randomness of policy $\pi$ and $H_t = (\Theta_t, A_t, u(\Theta_t, A_t, \Omega))$ is the triple of type, action and reward in round $t$ of policy $\pi$.

First of all, we have
\[
I(S'; \Omega| S) = I(R,H_1,...,H_{l-1}; \Omega| S)  = I(R; \Omega| S) + I(H_1,...,H_{l-1}; \Omega|S, R) = I(H_1,...,H_{l-1}; \Omega|S, R).
\]

By the chain rule of mutual information, we have
\[
 I(H_1,...,H_{l-1}; \Omega|S, R) = I(H_1;\Omega|S,R) + I(H_2;\Omega|S, R ,H_1) + \cdots + I(H_{l-1}; \Omega|S,R,H_1,...,H_{l-2}).
\]

For all $t \in [l-1]$, we have
\begin{align*}
&I(H_t; \Omega|S,R,H_1,...,H_{t-1}) \\
=& I(\Theta_t, A_t, u(\Theta_t, A_t, \Omega); \Omega|S,R,H_1,...,H_{t-1}) \\
=& I(\Theta_t ; \Omega|S,R,H_1,...,H_{t-1}) +  I(A_t, u(\Theta_t, A_t, \Omega); \Omega|S,R,H_1,...,H_{t-1},\Theta_t) \\
=& I(A_t, u(\Theta_t, A_t, \Omega); \Omega|S,R,H_1,...,H_{t-1},\Theta_t). \\
\end{align*}
Notice that the suggested action $A_t$ is a deterministic function of randomness of the recommendation policy $R$,  history of previous rounds $H_1,...,H_{t-1}$ and type in the current round $\Theta_t$. Also notice that, by induction hypothesis, $u(\Theta_t, A_t, \Omega)$ is a deterministic function of $|S,R,H_1,...,H_{t-1},\Theta_t, A_t$. Therefore we have
\[
I(H_t; \Omega|S,R,H_1,...,H_{t-1}) = 0, \forall t \in [l-1].
\]
Then we get
\[
I(S'; \Omega | S) = 0.
\]

By Lemma \ref{lem:infomono}, we know that $\EX[\S'] \subseteq \EX[\S]$. For state $\omega$, there exists a signal $s'$ such that $\Pr[S'=s'|\Omega =\omega] >0 $ and $a \in \EX_{s'} [\S']$. Now let $s$ be the realized value of $S$ given $\Omega = \omega$, we know that $\Pr[S'=s'|S=s] >0$, so $a \in \EX_s[\S]$. By Claim \ref{clm:maxexplore}, we know that at least one agent of type $\theta$ in phase $l$ of Algorithm \ref{alg:public_main} will choose action $a$.

Now we consider the case when $l > |\A| \cdot |\varTheta|$. Define Algorithm $E$ to be the variant of Algorithm \ref{alg:public_main} such that it only does exploration (removing the if-condition and exploitation in Algorithm \ref{alg:public_main}). For $l > |\A| \cdot |\varTheta|$, the above induction proof still work for Algorithm $E$, i.e. for a given state $\omega$, if an action $a$ of type $\theta$ can be explored by a BIC recommendation policy $\pi$ at round $l$, then such action is guaranteed to be explored by Algorithm $E$ by the end of phase $l$. Now we are going to argue that Algorithm $E$ won't explore any new action-type pairs after phase $|\A| \cdot |\varTheta|$. Call a phase exploring if in that phase Algorithm $E$ explores at least one new action-type pair. As there are  $ |\A| \cdot |\varTheta|$ type-action pairs, Algorithm $E$ can have at most $ |\A| \cdot |\varTheta|$ exploring phases. On the other hand, once Algorithm $E$ has a phase that is not exploring, because the signal stays the same after that phase, all phases afterwards are not exploring. Therefore Algorithm $E$ does not have any exploring phases after phase $|\A| \cdot |\varTheta|$. For $l > |\A| \cdot |\varTheta|$, the first $|   \A| \cdot |\varTheta|$ phases of Algorithm \ref{alg:public_main} explores the same set of type-action pairs as the first $l$ phases of Algorithm $E$. This concludes the proof.
\end{proof}

\begin{corollary}[Restatement of Theorem \ref{thm:public}]
\label{cor:public}
We have a BIC recommendation policy (Algorithm \ref{alg:public_main}) of $T$ rounds with expected total reward at least $\left(T - C \right) \cdot \OPT$ for constant $C = |\A| \cdot |\varTheta| \cdot \sum_{\theta\in\varTheta} \frac{L_{\theta}}{\Pr[\theta]}$.
\end{corollary}

\begin{proof}
First of all, by Claim \ref{clm:public_BIC}, Algorithm \ref{alg:public_main} is BIC.

By Lemma \ref{lem:exp_public}, for each state $\omega$, Algorithm \ref{alg:public_main} explores all the eventually explorable actions (i.e. $\A_{\omega,\theta}^{exp}$) for each type $\theta$ by the end of $|\A|\cdot |\varTheta|$ phases. After that, for each type $\theta$, Algorithm \ref{alg:public_main} will always recommend action $ \arg\max_{a \in \A_{\omega,\theta}^{exp}} u(\theta, a, \omega)$. Therefore Algorithm \ref{alg:public_main} gets reward $\OPT$ except rounds in first $|\A|\cdot |\varTheta|$ phases. By Lemma \ref{lem:epoch}, we know that the expected number of rounds of the first  $|\A|\cdot |\varTheta|$ phases is at most $C$. Therefore, Algorithm \ref{alg:public_main} has expected total reward at least  $\left(T - C \right) \cdot \OPT$.

\end{proof} 